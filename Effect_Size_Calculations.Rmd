---
title: "Quicker coding"
author: "Lewis Gourlay"
date: "2023-04-03"
output: html_document
editor_options: 
  chunk_output_type: console
---

#### Set-up of packages & Data

```{r Loading libraries, include=FALSE}
#set work directory
library(plyr)
library(dplyr)
library(ggplot2)
library(esc)
library(extrafont)
library(cowplot)
library(tinytest)
library(testthat)
library(patchwork)
library(ggdist)
library(ggthemes)
library(tidyquant)
library(tidyr)
library(metafor)
library(maps)
library(maptools)
library(ggstance)
library(summarytools)
```


```{r setting directory, include=FALSE}
# read-in data
zooALL<-read.csv("./data/dataset_final.csv",header=T, stringsAsFactors=FALSE, fileEncoding="latin1")

#rename Data_ID to Study for merging datasets
names(zooALL)[names(zooALL)=="Data_ID"]<-"study" 

#keep unedited version of the dataset
zooALL_copy = zooALL

#keep only unique rows
zooALL = unique(zooALL) #
zooALL_copy = unique(zooALL_copy)

#rename columns
zooALL %<>% dplyr::rename(es_old = "es")
zooALL %<>% dplyr::rename(se_old = "se")

# remove datapoints
zooALL<-zooALL[!is.na(zooALL$Value), ] # remove missing data
zooALL<-zooALL[zooALL$Value<25 & zooALL$Value>(-25), ]
```

```{r Convert environmental variables to the same scale, include=FALSE}

zooALL2<-zooALL #create copy dataset
zooALL2$Specific_scale[zooALL2$Specific_scale==""]<-1

zooALL2$Specific_scale <- as.factor(zooALL2$Specific_scale) #convert type to factor variables
zooALL2 <- subset(zooALL2,! zooALL2$Specific_scale  %in% c("ln(mm)","Log(degrees)","Log(mm)","mm^3/yr")) #remove non-numerical values
summary(zooALL2$Specific_scale)

zooALL3<-data.frame(Factorial = zooALL2$Specific_scale , Value = zooALL2$Value,Lower = zooALL2$Lower, Upper = zooALL2$Upper)
zooALL3$Upper<-as.numeric(zooALL3$Upper)
zooALL3$Lower<-as.numeric(zooALL3$Lower)

# write function for Value
convert_calculation_value <- function(factorial,value) {
  
  if(!is.na(factorial)) {
    return(value/as.numeric(as.character(factorial)))} else {value}
}

# Apply conversions
zooALL3$value_new<-(mapply(convert_calculation_value, zooALL3$Factorial,zooALL3$Value))
zooALL3$Upper_new<-mapply(convert_calculation_value,zooALL3$Factorial,zooALL3$Upper)
zooALL3$Lower_new<-mapply(convert_calculation_value,zooALL3$Factorial,zooALL3$Lower)

length(mapply(convert_calculation_value, zooALL3$Factorial,zooALL3$Value)) 
length(mapply(convert_calculation_value,zooALL3$Factorial,zooALL3$Upper)) 
length(mapply(convert_calculation_value,zooALL3$Factorial,zooALL3$Lower)) 

length(zooALL3$value_new) 
length(zooALL3$Upper_new) 
length(zooALL3$Lower_new) 

##unlist and add on
### unlist 2 to deal with NULL
unlist2<-function(a.list) {
  
  return(do.call(c, lapply(a.list, (function(x) {
  if (is.null(x) | length(x) == 0) {NA} else { x } 
}))))
  
}
  
zooALL2$Value<-unlist2(zooALL3$value_new)
zooALL2$Upper<-unlist2(zooALL3$Upper_new)
zooALL2$Lower<-unlist2(zooALL3$Lower_new)

zooALL<-zooALL2 #use dataset which contains data using scales stated numerically
```

```{r composition of final dataset}

# Count the number of independent studies
num_studies <- zooALL %>%
  filter(!is.na(Reference_ID)) %>%
  summarise(num_studies = n_distinct(Reference_ID))

# Print the result
cat("Number of independent studies:",num_studies$num_studies, "\n")


```

```{r Function to convert standard error to standard deviation, include = TRUE}
# define the function to test
se_to_sd <- function(se, n) {
  se * sqrt(n)
}
```

```{r Function to convert 95% Confidence interval to Standard deviation, include = TRUE}
# define the function to test
ci_to_sd<-function(ci_lower, ci_upper, n, conf_level = 0.95) {
  crit_val <- qnorm((1 + conf_level) / 2)
  sqrt(n) * (ci_upper - ci_lower) / (2 * crit_val)
}
```

#### Effect sizes

```{r Effect sizes for Odds Ratio, include = TRUE}
#Subset data
OR.1<-subset(zooALL, zooALL$Type %in% c("Odds ratio"))
OR<-subset(OR.1, OR.1$Error %in% c("95% Confidence"))

#Change character columns to numeric
OR$Std._error<-as.numeric(OR$Std._error)
OR$Value<-as.numeric(OR$Value) 
OR$N.value<-as.numeric(OR$N.value) 
OR$Upper<-as.numeric(OR$Upper)
OR$Lower<-as.numeric(OR$Lower) 

#Conversion from  95% Confidence interval to Standard error
OR$Std._error<-(OR$Upper - OR$Lower) / (2 * 1.96)

#Calculate effect sizes
OR$N.value[is.na(OR$N.value)]<-median(OR$N.value,na.rm=TRUE) #assign median N.value to missing datapoints

# Calculating Hedge's g
Hedge_OR <- do.call(rbind, lapply(1:nrow(OR), function(i) {
  convert_or2d(
    or = OR$Value[i], 
    se = OR$Std._error[i], 
    totaln = OR$N.value[i], 
    es.type = "g", 
    study = OR$study[i]
  )
}))

# Converting to a dataframe
Hedge_OR <- as.data.frame(Hedge_OR)

Hedge_OR<-Hedge_OR[!is.na(Hedge_OR$se),]#Hedge's g for logistic regression models
#OR$study
Hedge_OR$study = unlist(Hedge_OR$study)
OR_comp<-left_join(OR,Hedge_OR, by = "study")

View(OR_comp %>% select(study, es, es_old))
### Attempt to convert Odds ratio with no error to Hedge's G effect size
OR_t <- subset(OR.1, OR.1$Country %in% c("Australia"))

# calculating effect size using alternative formula
Hedge_OR_t <-as.data.frame(log(OR_t$Value) * sqrt(3) / pi)


Hedge_OR_t <-cbind(OR_t$study, Hedge_OR_t)
names(Hedge_OR_t)[names(Hedge_OR_t)=="log(OR_t$Value) * sqrt(3)/pi"]<-"es"
names(Hedge_OR_t)[names(Hedge_OR_t)=="OR_t$study"]<-"study"

# Plotting graph

OR_comp_scatter<-ggplot(data = OR_comp, aes(x=Value, y=es))+geom_point()+
  xlab("Odds Ratio")+ylab("Effect size (g)")+theme_classic()+geom_line() +geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE) #scatter showing linear relationship between correlation coefficient and effect size

# Assess outlier using residuals
model <- lm(es~ Value, data = OR_comp)
residuals <- model$residuals
outliers <- OR_comp[abs(residuals) > 2*sd(residuals), ]

OR_comp_scatter<-OR_comp_scatter+ geom_point(data = outliers, aes(x=outliers$Value, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

OR_comp_scatter
```



```{r Effect sizes for Beta coefficient, include=TRUE, error=TRUE}
Beta<-subset(zooALL,zooALL$Type %in% c("Beta coefficient")) #subset data

#Convert 95% Confidence interval to Standard deviation
Beta_95<-subset(Beta, Beta$Error %in% c("95% Confidence","95% Credible"))

Beta$Value<-as.numeric(Beta$Value) #converting type to numeric
Beta_95$Upper<-as.numeric(Beta_95$Upper) #converting type to numeric
Beta_95$Lower<-as.numeric(Beta_95$Lower) #converting type to numeric
Beta_95$N.value<-as.numeric(Beta_95$N.value) #converting type to numeric
Beta$Std._error<-as.numeric(Beta$Std._error) #converting type to numeric
Beta_95_sd<-as.data.frame((sqrt(Beta_95$N.value) * (Beta_95$Upper - Beta_95$Lower))/3.92) #Std. deviation from 95% CI
Beta_95<-cbind(Beta_95,Beta_95_sd)
names(Beta_95)[names(Beta_95)=="(sqrt(Beta_95$N.value) * (Beta_95$Upper - Beta_95$Lower))/3.92"]<-"Std_dev"

#Convert Standard error to Standard Deviation
Beta_SE<-(subset(Beta,Beta$Error %in% ("Std. error")))

Beta_SE$Upper<-as.numeric(Beta_SE$Upper) #change character to numeric
Beta_SE$Lower<-as.numeric(Beta_SE$Lower)#change character to numeric
Beta_SE$N.value<-as.numeric(Beta_SE$N.value) #change character to numeric
Beta_SE$Std._error<-as.numeric(Beta_SE$Std._error)
Beta_SE.1<-as.data.frame(Beta_SE$Std._error * sqrt(Beta_SE$N.value)) #Std. deviation from Std. error
Beta_SE<-cbind(Beta_SE,Beta_SE.1)
names(Beta_SE)[names(Beta_SE)=="Beta_SE$Std._error * sqrt(Beta_SE$N.value)"]<-"Std_dev"

#Calculate Hedge's G effect size
Beta_Stdev <-rbind(Beta_95, Beta_SE)
Hedge_Beta<-data.frame(esc_beta(beta = Beta_Stdev$Value, sdy = Beta_Stdev$Std_dev, grp1n = Beta_Stdev$N.value/2, grp2n = Beta_Stdev$N.value/2, es.type = ("g"),study = Beta_Stdev$study)) 
Hedge_Beta<-Hedge_Beta[!is.na(Hedge_Beta$se),]

#Calculating the Hedge's G of beta coefficients without error to see if they are outliers or can be used
Beta_t<-subset(Beta, !Beta$Error %in% c("95% Confidence", "Std. error", "95% Credible"))

Beta_d<-as.data.frame(cohens_d(Beta_t$Value)) #calculate cohen's effect size


Beta_t1<-cbind(Beta_t, Beta_d) #Bind Dataframe and Cohen's D effect size calculation
summary(Beta_t1)
names(Beta_t1)[names(Beta_t1)=="cohens_d(Beta_t$Value)"]<-"d"

Hedge_Beta_t<-as.data.frame(hedges_g(d=Beta_t1$d, totaln =Beta_t1$N.value)) #Calculate hedge G from Cohen's D

summary(Hedge_Beta_t)
names(Hedge_Beta_t)[names(Hedge_Beta_t)=="hedges_g(d = Beta_t1$d, totaln = Beta_t1$N.value)"]<-"es" #change name to 'es'
Beta_t2<-cbind(Beta_t$study, Hedge_Beta_t)
names(Beta_t2)[names(Beta_t2)=="Beta_t$study"]<-"study" #change name to 'es'

Hedge_Beta_t<-Beta_t2

#Unit scaling and Sanity checks
Beta_comp<-merge(Beta_Stdev, Hedge_Beta, by="study")
names(Beta_comp)[names(Beta_SE)=="Value"]<-"Correlation"
Beta_comp_scatter<-ggplot(data = Beta_comp, aes(x=Value, y=es,label=study))+geom_point()+
  xlab("Correlation coefficient (β)")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE)

# Assess outlier using residuals
model <- lm(es~ Value, data = Beta_comp)
residuals <- model$residuals
outliers <- Beta_comp[abs(residuals) > 2*sd(residuals), ]

Beta_comp_scatter<-Beta_comp_scatter+ geom_point(data = outliers, aes(x=outliers$Value, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size


Beta_comp_scatter

#graphing Beta coefficients which don't have errors
Beta_t_comp<-merge(Beta_t, Beta_t2, by="study")
Beta_t_comp_scatter<-ggplot(data = Beta_t_comp, aes(x=Value, y=es,label=study))+geom_point()+
  xlab("Correlation coefficient (β)")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE)

Beta_t_comp_scatter

### Test for Standard error
Beta_test_se<-subset(Beta_comp,Beta_comp$Error %in% c("Std. error"))
Beta_test_se <- sample_n(Beta_test_se, 5)

Beta_test_se$Std._error<-as.numeric(Beta_test_se$Std._error)
Beta_test_se$N.value<-as.numeric(Beta_test_se$N.value)
Beta_test_se$Std_dev<-as.numeric(Beta_test_se$Std_dev)

test_that("se_to_sd returns correct standard deviation for all rows in a dataframe", {
  
  Beta_test_se<-data.frame(se=Beta_test_se$Std._error,
                           n=Beta_test_se$N.value,
                           sd = Beta_test_se$Std_dev)
  
  expected_sds <- Beta_test_se$sd
  actual_sds <- apply(Beta_test_se, 1, function(x) se_to_sd(x[1], x[2]))
  expect_equal(round(actual_sds,3), round(expected_sds,3))
})

### Test for 95% CI
Beta_test_ci <- subset(Beta_comp, Error %in% c("95% Confidence"))
Beta_test_ci <- sample_n(Beta_test_ci, 5)

# Test the function
test_that("ci_to_sd returns correct standard deviation for all rows", {
  # Create the data frame
  data <- data.frame(
    ci_lower = Beta_test_ci$Lower,
    ci_upper = Beta_test_ci$Upper,
    sd = Beta_test_ci$Std_dev,
    n = Beta_test_ci$N.value
  )
  
  # Calculate the expected and actual standard deviations
  expected_sds <- data$sd
  actual_sds <- apply(data, 1, function(x) ci_to_sd(x[1], x[2], x[4]))
  
  # Compare the expected and actual standard deviations
  expect_equal(actual_sds, expected_sds)
})
```


```{r Effect sizes for Unstandardised coefficients, include=TRUE}
B<-(subset(zooALL, zooALL$Type %in% c("Correlation coefficient"))) #subset
B<-(subset(B,!B$Statistical_method %in% c("Spearman rank correlation", "Pearson correlation"))) #remove rank correlations

#95% Confidence interval conversion
B_95<-(subset(B,B$Error %in% c("95% Confidence", "95% Credible")))
B_95$Upper<-as.numeric(B_95$Upper) #converting type to numeric
B_95$Lower<-as.numeric(B_95$Lower) #converting type to numeric
B_95$N.value<-as.numeric(B_95$N.value) #converting type to numeric
B$Std._error<-as.numeric(B$Std._error) #converting type to numeric
B_95_sd<-as.data.frame((sqrt(B_95$N.value) * (B_95$Upper - B_95$Lower))/3.92) #Std. deviation from 95% CI
B_95<-cbind(B_95,B_95_sd)
names(B_95)[names(B_95)=="(sqrt(B_95$N.value) * (B_95$Upper - B_95$Lower))/3.92"]<-"Std_dev"

#Standard error conversion
B_SE<-(subset(B,B$Error %in% ("Std. error"))) #subset data to those with a SE
B_SE$Upper<-as.numeric(B_SE$Upper) #change character to numeric
B_SE$Lower<-as.numeric(B_SE$Lower)#change character to numeric
B_SE$N.value<-as.numeric(B_SE$N.value) #change character to numeric
B_SE$Std._error<-as.numeric(B_SE$Std._error)
B_SE.1<-as.data.frame(B_SE$Std._error * sqrt(B_SE$N.value)) #Std. deviation from Std. error
B_SE<-cbind(B_SE,B_SE.1)
names(B_SE)[names(B_SE)=="B_SE$Std._error * sqrt(B_SE$N.value)"]<-"Std_dev"



#Calculate hedge's G effect size
B_Stdev<-rbind(B_95, B_SE)
B_Stdev$N.value[is.na(B_Stdev$N.value)]<-mean(B_Stdev$N.value,na.rm=TRUE)
Hedge_B<-as.data.frame(esc_B(b = B_Stdev$Value, sdy = B_Stdev$Std_dev, grp1n = B_Stdev$N.value/2, grp2n = B_Stdev$N.value/2, es.type =c("g"),study = B_Stdev$study)) #calculate hedge g

#Unit scaling and Sanity checks
B_comp<-as.data.frame(merge(B_Stdev, Hedge_B, by="study"))
names(B_comp)[names(B_SE)=="Value"]<-"Correlation"
B_comp_scatter<-ggplot(data = B_comp, aes(x=Value, y=es,label=as.character(N.value)))+geom_point()+xlab("Correlation coefficient")+ylab("Effect size (g)")+
theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
geom_smooth(method = "lm", se = FALSE) #scatter showing linear relationship between correlation coefficient and effect size

# Assess outlier using residuals
model <- lm(es~ Value, data = B_comp)
residuals <- model$residuals
outliers <- B_comp[abs(residuals) > 2*sd(residuals), ]

B_comp_scatter<-B_comp_scatter+ geom_point(data = outliers, aes(x=outliers$Value, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

B_comp_scatter

### Test for Standard error
B_test_se<-subset(B_comp,B_comp$Error %in% c("Std. error"))
B_test_se <- sample_n(B_test_se, 5)

B_test_se$Std._error<-as.numeric(B_test_se$Std._error)
B_test_se$N.value<-as.numeric(B_test_se$N.value)
B_test_se$Std_dev<-as.numeric(B_test_se$Std_dev)


test_that("se_to_sd returns correct standard deviation for all rows in a dataframe", {
  
  B_test_se<-data.frame(se=B_test_se$Std._error,
                           n=B_test_se$N.value,
                           sd = B_test_se$Std_dev)
  
  expected_sds <- B_test_se$sd
  actual_sds <- apply(B_test_se, 1, function(x) se_to_sd(x[1], x[2]))
expect_equal(round(actual_sds,3), round(expected_sds,3))
})


### Test for 95% CI
B_test_ci <- subset(B_comp, Error %in% c("95% Confidence"))

# Test the function
test_that("ci_to_sd returns correct standard deviation for all rows", {
  # Create the data frame
  data <- data.frame(
    ci_lower = B_test_ci$Lower,
    ci_upper = B_test_ci$Upper,
    sd = B_test_ci$Std_dev,
    n = B_test_ci$N.value
  )
  
  # Calculate the expected and actual standard deviations
  expected_sds <- data$sd
  actual_sds <- apply(data, 1, function(x) ci_to_sd(x[1], x[2], x[4]))
  
  # Compare the expected and actual standard deviations
expect_equal(round(actual_sds,2), round(expected_sds,2))
})
```


```{r Effect sizes for Spearman & Pearson correlaiton, include=TRUE}
R<- subset(zooALL, zooALL$Statistical_method %in% c("Spearman rank correlation", "Pearson correlation")) #pearson and spearmans
B.1<-subset(B,!B$Error %in% c("95% Confidence", "Std. error")) #correlation coefficients without error values
R.1<-rbind(R, B.1)
R_d<-cohens_d(R.1$Value) # calculate cohen's effect size #ARTUR COMMENT: shouldnt it be cohens_d(r = R.1$Value) instead?
                         # this function takes f as default, but we have r (correlation coefficient)

R.1$N.value<-as.numeric(R.1$N.value) 
R.2<-cbind(R.1,R_d) #Bind Dataframe and Cohen's D effect size calculation

Hedge_R<-as.data.frame(hedges_g(d=R.2$R_d, totaln = R.2$N.value)) #Calculate hedge G from Cohen's D

names(Hedge_R)[names(Hedge_R)=="hedges_g(d = R.2$R_d, totaln = R.2$N.value)"]<-"es" #change name to 'es'
R_comp<-cbind(R.2, Hedge_R)
names(Hedge_R)[names(Hedge_R)=="R$study"]<-"study" #change name to 'study' to allow datasets to be combined later 

Hedge_R<-cbind(R.2$study, Hedge_R)
names(Hedge_R)[names(Hedge_R)=="R.2$study"]<-"study"

#Unit scaling and Sanity checks

names(R_comp)[names(R_comp)=="Value"]<-"Correlation"
R_comp_scatter<-ggplot(data = R_comp,aes(x=R_comp$Correlation, y=es))+geom_point()+xlab("Correlation coefficient (r)")+ylab("Effect size (g)")+
  geom_line()+theme_classic()+
geom_smooth(method = "lm", se = FALSE)
R_comp_scatter #scatter showing linear relationship between correlation coefficient and effect size 


# Assess outlier using residuals
model <- lm(es~ Correlation, data = R_comp)
residuals <- model$residuals
outliers <- R_comp[abs(residuals) > 2*sd(residuals), ]

R_comp_scatter<-R_comp_scatter+ geom_point(data = outliers, aes(x=outliers$Correlation, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

R_comp_scatter
```

```{r Effect sizes for Ratios, include=TRUE}

Ratios<-subset(zooALL,zooALL$Type %in% c("Relative risk", "Risk ratio","Incidence rate ratio","Prevelance ratio","Hospitilisation rate ratio")) #subset

#convert 95% confidence intervals to Standard error
Ratios<-(subset(Ratios, Ratios$Error %in% c("95% Confidence","95% Credible")))

Ratios$Value<-as.numeric(Ratios$Value) #convert to numeric
Ratios$N.value<-as.numeric(Ratios$N.value) #convert to numeric
Ratios$Upper<-as.numeric(Ratios$Upper) #convert to numeric
Ratios$Lower<-as.numeric(Ratios$Lower) #convert to numeric

Ratios$Std._error<-(Ratios$Upper - Ratios$Value)/3.92 #finding SE
names(Ratios)[names(Ratios)=="Ratios_SE"]<-"SE"
Ratios<-Ratios[complete.cases(Ratios$Upper), ]

##reasonable?
Ratios$N.value[is.na(Ratios$N.value)]<-median(Ratios$N.value,na.rm=TRUE)

Hedge_Ratios<-as.data.frame(convert_or2d(or =Ratios$Value, se = Ratios$Std._error, totaln = Ratios$N.value, es.type = c("g"), study = Ratios$study))

##remove errors
Hedge_Ratios<-Hedge_Ratios[Hedge_Ratios$se!=0,]

#Unit scaling and Sanity checks
Ratios_comp<-merge(Ratios,Hedge_Ratios, by = "study")
Ratios_comp_scatter<-ggplot(data = Ratios_comp, aes(x=Value, y=es))+geom_point()+
  xlab("Ratios")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_smooth(method = "lm", se = FALSE)
Ratios_comp_scatter #scatter showing linear relationship between correlation coefficient and effect size  

# Assess outlier using residuals
model <- lm(es~ Value, data = Ratios_comp)
residuals <- model$residuals
outliers <- Ratios_comp[abs(residuals) > 2*sd(residuals), ]

Ratios_comp_scatter<-Ratios_comp_scatter +geom_point(data = outliers, aes(x=outliers$Value, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

Ratios_comp_scatter
```


```{r Effect sizes for Z-scores, include=TRUE}
z<-(subset(zooALL, zooALL$Type %in% c("Z-score")))
z$N.value<-as.numeric(z$N.value)

#reasonable?
#z$N.value[is.na(z$N.value)]<-median(z$N.value,na.rm=TRUE)

Hedge_z<-convert_z2r(z$Value)
Hedge_z<-cohens_d(Hedge_z) # ARTUR: SHOULDN'T IT BE cohens_d(r = Hedge_z)?
                           # by default that function takes f.
Hedge_z<-as.data.frame(hedges_g(d=Hedge_z, totaln = z$N.value))
Hedge_z

Hedge_z<-cbind(z$study, Hedge_z)
names(Hedge_z)[names(Hedge_z)=="z$study"]<-"study"
names(Hedge_z)[names(Hedge_z)=="hedges_g(d = Hedge_z, totaln = z$N.value)"]<-"es"
Hedge_z

Hedge_z_old = zooALL_copy %>% filter(Data_ID %in% Hedge_z$study) %>% select(Data_ID, es, Value)
Hedge_z
Hedge_z_old
#Unit scaling and Sanity checks
z_comp<-merge(z,Hedge_z, by = "study")

```

```{r Effect sizes for T-test, include=TRUE}
t<-(subset(zooALL, zooALL$Type %in% c("T-value")))
Hedge_t<-as.data.frame(esc_t(t=2.24, p=0.05, totaln = 8917, es.type = c("g"), study = t$study))
```

```{r Effect sizes for Chi-square, include=TRUE}
Chi<-subset(zooALL, zooALL$Type %in% ("Chi-square")) #Subset
Hedge_chi<-as.data.frame(esc_chisq(chisq = 4.75, p = 0.03, totaln = 3006, es.type = c("g"), study = Chi$study)) # Effect size Hedge's g
```


```{r Effect sizes for  F-statistic, include=TRUE}
FS<-subset(zooALL, zooALL$Type %in% ("F-statistic")) #subset f-statistics

FS$Value<-as.numeric(FS$Value)
FS$N.value<-as.numeric(FS$N.value)
Hedge_F<-as.data.frame(esc_f(f = FS$Value, totaln = FS$N.value, es.type = c("g"),study = FS$study)) # Effect size Hedge's g

#Unit scaling and Sanity checks
F_comp<-merge(FS,Hedge_F, by = "study")
F_comp_scatter<-ggplot(data = F_comp, aes(x=Value, y=es))+geom_point()+
  xlab("F-statistic")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_smooth(method = "lm", se = FALSE) #scatter showing linear relationship between correlation coefficient and effect size

# Assess outlier using residuals
model <- lm(es~ Value, data = F_comp)
residuals <- model$residuals
outliers <- F_comp[abs(residuals) > 2*sd(residuals), ]

F_comp_scatter<-F_comp_scatter+geom_point(data = outliers, aes(x=outliers$Value, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

F_comp_scatter
```
```{r Effect sizes for Percentage change, include=TRUE, error=TRUE}
p<-subset(zooALL,zooALL$Type %in% "Percent change") #subset

p$B<-(p$Value/100)#convert % change to a rough b coefficient
p$L<-(p$Lower/100) #convert % change error to a rough L limit
p$U<-(p$Upper/100) #convert % change error to a rough U limit

#Convert 95% Confidence interval to Standard deviation
p_95<-subset(p, p$Error %in% c("95% Confidence"))

p$B<-as.numeric(p$B) #converting type to numeric
p_95$U<-as.numeric(p_95$U) #converting type to numeric
p_95$L<-as.numeric(p_95$L) #converting type to numeric
p_95$N.value<-as.numeric(p_95$N.value) #converting type to numeric
p$Std._error<-as.numeric(p$Std._error) #converting type to numeric
p_95_sd<-as.data.frame((sqrt(p_95$N.value) * (p_95$U - p_95$L))/3.92) #Std. deviation from 95% CI
p_95<-cbind(p_95,p_95_sd)
summary(p_95)
names(p_95)[names(p_95)=="(sqrt(p_95$N.value) * (p_95$U - p_95$L))/3.92"]<-"Std_dev"

#Convert Standard error to Standard Deviation
p_SE<-(subset(p,p$Error %in% ("Std. error")))

p_SE$U<-as.numeric(p_SE$U) #change character to numeric
p_SE$L<-as.numeric(p_SE$L)#change character to numeric
p_SE$N.value<-as.numeric(p_SE$N.value)
p_SE$Std._error<-as.numeric(p_SE$Std._error)

p_SE.1<-as.data.frame(p_SE$Std._error * sqrt(p_SE$N.value)) #Std. deviation from Std. error
p_SE<-cbind(p_SE,p_SE.1)
summary(p_SE)
names(p_SE)[names(p_SE)=="p_SE$Std._error * sqrt(p_SE$N.value)"]<-"Std_dev"

#Calculate Hedge's G effect size
p_Stdev <-rbind(p_95, p_SE)

Hedge_p<-as.data.frame(esc_beta(beta = p_Stdev$B, sdy = p_Stdev$Std_dev, grp1n = p_Stdev$N.value/2, grp2n = p_Stdev$N.value/2, es.type = "g",study = p_Stdev$study))

Hedge_p<-Hedge_p[!is.na(Hedge_p$se),]

#Unit scaling and Sanity checks
p_comp<-merge(p_Stdev, Hedge_p, by="study")
names(p_comp)[names(p_SE)=="B"]<-"Correlation"
p_comp_scatter<-ggplot(data = p_comp, aes(x=Correlation, y=es,label=study))+geom_point()+
  xlab("Correlation coefficient")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE)


# Assess outlier using residuals
model <- lm(es~ p_comp$Correlation, data = p_comp)
residuals <- model$residuals
outliers <- p_comp[abs(residuals) > 2*sd(residuals), ]

p_comp_scatter<-p_comp_scatter+ geom_point(data = outliers, aes(x=outliers$Correlation, y=outliers$es,color="red",size =3)) #scatter showing linear relationship between correlation coefficient and effect size

p_comp_scatter

### Test for Standard error
p_test_se<-subset(p_comp,p_comp$Error %in% c("Std. error"))
p_test_se <- sample_n(p_test_se,size = 3)

p_test_se$Std._error<-as.numeric(p_test_se$Std._error)
p_test_se$N.value<-as.numeric(p_test_se$N.value)
p_test_se$N.value
p_test_se$Std_dev<-as.numeric(p_test_se$Std_dev)

test_that("se_to_sd returns correct standard deviation for all rows in a dataframe", {
  
  p_test_se<-data.frame(se=p_test_se$Std._error,
                           n=p_test_se$N.value,
                           sd = p_test_se$Std_dev)
  
  expected_sds <- p_test_se$sd
  actual_sds <- apply(p_test_se, 1, function(x) se_to_sd(x[1], x[2]))
  expect_equal(round(actual_sds,3), round(expected_sds,3))
})

### Test for 95% CI
p_test_ci <- subset(p_comp, Error %in% c("95% Confidence"))
p_test_ci <- sample_n(p_test_ci, 5)

# Test the function
test_that("ci_to_sd returns correct standard deviation for all rows", {
  # Create the data frame
  data <- data.frame(
    ci_L = p_test_ci$L,
    ci_U = p_test_ci$U,
    sd = p_test_ci$Std_dev,
    n = p_test_ci$N.value
  )
  
  # Calculate the expected and actual standard deviations
  expected_sds <- data$sd
  actual_sds <- apply(data, 1, function(x) ci_to_sd(x[1], x[2], x[4]))
  
  # Compare the expected and actual standard deviations
  expect_equal(actual_sds, expected_sds)
})
```


```{r Effect sizes for R-squared, include=TRUE, error=TRUE}
R_square<-subset(zooALL, zooALL$Type %in% "R square")

# Convert R-squared to Cohen's f^2 effect size
f2 <- R_square$R.squared / (1 - R_square$R.squared)

# Convert Cohen's f^2 to Hedge's g effect size
g<-as.data.frame(sqrt(f2) * 1.5)


Hedge_R_square<-cbind(R_square$study, g, R_square$Direction) 

names(Hedge_R_square)[names(Hedge_R_square)=="sqrt(f2) * 1.5"]<-"es"
names(Hedge_R_square)[names(Hedge_R_square)=="R_square$study"]<-"study"
names(Hedge_R_square)[names(Hedge_R_square)=="R_square$Direction"]<-"Direction"

Hedge_R_square$es[Hedge_R_square$Direction =="decrease"] <- -Hedge_R_square$es[Hedge_R_square$Direction == "decrease"] #change sign of effect size based on the direction of relationship

Hedge_R_square<-Hedge_R_square[,-3]

#Unit scaling and Sanity checks
Rsquare_comp<-merge(Hedge_R_square, R_square, by="study")

Rsquare_comp_scatter<-ggplot(data = Rsquare_comp, aes(x=Value, y=es,label=study))+geom_point()+
  xlab("R-squared")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE)

# Assess outlier using residuals
model <- lm(es~ Rsquare_comp$Value, data = Rsquare_comp)
residuals <- model$residuals
outliers <- Rsquare_comp[abs(residuals) > 2*sd(residuals), ]

Rsquare_comp_scatter
```

```{r Effect sizes for Bivariate regression analysis estimate, include=TRUE, error=TRUE}
Est<-subset(zooALL, zooALL$Type %in% "Estimate")

Hedge_est<-as.data.frame(hedges_g(d=Est$Value, totaln=Est$N.value))

Hedge_est<-cbind(Est$study, Hedge_est)

summary(Hedge_est)
names(Hedge_est)[names(Hedge_est)=="hedges_g(d = Est$Value, totaln = Est$N.value)"]<-"es"
names(Hedge_est)[names(Hedge_est)=="Est$study"]<-"study"

Est_comp<-merge(Est, Hedge_est, by="study")
summary(Est_comp)


Est_comp_scatter<-ggplot(data = Est_comp, aes(x=Value, y=es,label=study))+geom_point()+
  xlab("Estimate")+ylab("Effect size (g)")+theme_classic()+geom_line()+geom_text(aes(label = study),vjust = 1)+
  geom_smooth(method = "lm", se = FALSE)

Est_comp_scatter
```
#### Graphing effect sizes
```{r Plotting the general effect size trends, include=TRUE}
Hedge_all<-rbind(Hedge_Beta, Hedge_B, Hedge_F, Hedge_chi, Hedge_OR, Hedge_t, Hedge_p,Hedge_Ratios)
Hedge_all<-rbind.fill(Hedge_all, Hedge_z, Hedge_R, Hedge_R_square, Hedge_est, Hedge_Beta_t,Hedge_OR_t)
Hedge_all$n_method<-c(rep(nrow(Hedge_chi),nrow(Hedge_chi)),rep(nrow(Hedge_B),nrow(Hedge_B)),rep(nrow(Hedge_Beta),nrow(Hedge_Beta)),rep(nrow(Hedge_F),nrow(Hedge_F)),rep(nrow(Hedge_t),nrow(Hedge_t)),rep(nrow(Hedge_OR),nrow(Hedge_OR)),rep(nrow(Hedge_z),nrow(Hedge_z)),rep(nrow(Hedge_R),nrow(Hedge_R)),rep(nrow(Hedge_p),nrow(Hedge_p)),rep(nrow(Hedge_R_square),nrow(Hedge_R_square)),rep(nrow(Hedge_est),nrow(Hedge_est)),rep(nrow(Hedge_Beta_t),nrow(Hedge_Beta_t)),rep(nrow(Hedge_OR_t),nrow(Hedge_OR_t)),rep(nrow(Hedge_Ratios),nrow(Hedge_Ratios)))

#merge together
zooALL_effect_size <- merge(zooALL,Hedge_all, by="study")
zooALL_effect_size<-zooALL_effect_size[!is.na(zooALL_effect_size$es),]

#zooALL_effect_size<-zooALL_effect_size[zooALL_effect_size$n_method>1,]


zooALL_effect_size$Environmental_condition<-as.factor(zooALL_effect_size$Environmental_condition) #convert type to factor

zooALL_es_temp<-subset(zooALL_effect_size,zooALL_effect_size$Environmental_condition %in% ("Temperature")) #subset temperature
zooALL_es_prec<-subset(zooALL_effect_size,zooALL_effect_size$Environmental_condition %in% ("Precipitation"))#subset precipitation
zooALL_es_hum<-subset(zooALL_effect_size,zooALL_effect_size$Environmental_condition %in% ("Humidity")) #subset humidity

summary(zooALL_es_temp) #summarise
summary(zooALL_es_prec) #summarise
summary(zooALL_es_hum) #summarise


#zooALL_effect_size$vector[zooALL_effect_size$vector==""]<-""

##create groupings
zooALL_effect_size$group1<-zooALL_effect_size$Environmental_condition
zooALL_effect_size$group2<-paste(zooALL_effect_size$Environmental_condition,zooALL_effect_size$Type,sep="_")
zooALL_effect_size$group3<-paste(zooALL_effect_size$Environmental_condition,zooALL_effect_size$type,sep="_")
zooALL_effect_size$group4<-paste(zooALL_effect_size$Environmental_condition,zooALL_effect_size$type,zooALL_effect_size$vector, sep="_")
zooALL_effect_size$group5<-paste(zooALL_effect_size$Environmental_condition,zooALL_effect_size$P_V_B,sep="_")
zooALL_effect_size$group6<-paste(zooALL_effect_size$Environmental_condition,zooALL_effect_size$type,zooALL_effect_size$P_V_B, sep="_")


###chose one
#zooALL_effect_size$Grouping<-zooALL_effect_size$group1
#zooALL_effect_size$Grouping<-zooALL_effect_size$group2
#zooALL_effect_size$Grouping<-zooALL_effect_size$Type
zooALL_effect_size$Grouping<-zooALL_effect_size$group1

summary(zooALL_effect_size)

#View(zooALL_effect_size)

#Observing the distribution of effect sizes
range_effect_size <- 4*sd(zooALL_effect_size$es) #standard deviation of effect sizes

cat("Expected minimum effect size:", -range_effect_size, "\n") #estimate
cat("Expected maximum effect size:", range_effect_size, "\n") #estimate

n_distinct(zooALL_effect_size$study)

# Set the number of studies and observations
k <- 173
n <- 556

# Calculate the sampling variance
v <- (n - 1) / (n - k)

# Calculate the expected range of effect sizes
lower <- qnorm(0.025, lower.tail = FALSE) * sqrt(v)
upper <- qnorm(0.975, lower.tail = FALSE) * sqrt(v)

# Print the results
cat("Expected range of effect sizes:", round(lower, 3), "to", round(upper, 3))
```

# Alternative calculation of Effect Sizes 

## Z-score

```{r}
z<-(subset(zooALL, zooALL$Type %in% c("Z-score")))
z$N.value<-as.numeric(z$N.value)

#reasonable?
#z$N.value[is.na(z$N.value)]<-median(z$N.value,na.rm=TRUE)

#Hedge_z<-convert_z2r(z$Value)
#Hedge_z<-cohens_d(r = Hedge_z) # ARTUR: SHOULDN'T IT BE cohens_d(r = Hedge_z)?
                           # by default that function takes f.
Hedge_z<-as.data.frame(hedges_g(d=z$Value, totaln = z$N.value))
Hedge_z

Hedge_z<-cbind(z$study, Hedge_z)
names(Hedge_z)[names(Hedge_z)=="z$study"]<-"study"
names(Hedge_z)[names(Hedge_z)=="hedges_g(d = Hedge_z, totaln = z$N.value)"]<-"es"
Hedge_z

Hedge_z_old = zooALL %>% filter(study %in% Hedge_z$study) %>% select(study, es_old, Value, N.value, Reference_ID)
Hedge_z
Hedge_z_old
merged_Z = left_join(Hedge_z_old, Hedge_z, by = "study")

```

## Estimate

```{r}

```

## Spearman/Pearson correlation

```{r}

```

