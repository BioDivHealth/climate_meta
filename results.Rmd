---
title: "Climate impacts on zoonotic disease meta-analysis results"
output: html_document
---

```{r setup, include=FALSE, results='asis'}
# deps
library(tidyr)
library(twosamples)
library(magrittr)
library(tidyverse)

knitr::opts_chunk$set(echo = FALSE)

# load dataset
df = read.csv('data/dataset_final.csv')

# change vector names
df = df%>%
  mutate(Transmission_type = ifelse(Transmission_type == 'HVH', 'Vectored', 'Non-vectored'))

# add parasite group
df = df%>%
  mutate(Pathogen = ifelse(Pathogen == 'P', 'Parasite',
                                ifelse(Pathogen == 'V', 'Virus',
                                        'Bacteria')))

cat(paste('Number of unique studies:', length(unique(df$Reference_ID))))
cat(paste('Number of pathogens:', length(unique(df$Disease))))
cat(paste('Number of countries:', length(unique(df$Country))))
```

# Results Section 1: Literature search results

**Climate variables assessed - 3 categories**

```{r climatevars1}
 df%>%
  select(Reference_ID, Environmental_condition)%>%
  distinct() %>% 
  count(Environmental_condition) %>%  
  group_by(Environmental_condition) %>% 
  mutate(prop = n/length(unique(df$Reference_ID)))
```

**Climate variables assessed**

```{r climatevars2}

df%>%
  select(Reference_ID, Environmental_condition)%>%
  distinct()%>%
  mutate(measured = Environmental_condition)%>%
  mutate(measured = ifelse(measured == 'Temperature', 'T', ifelse(measured == 'Precipitation', 'P', 'H')))%>%
  pivot_wider(names_from=Environmental_condition, values_from=measured ,values_fill='')%>%
  relocate(any_of(c('Reference_ID', 'Temperature', 'Precipitation', 'Humidity')))%>%
  mutate(ClimMeasured = paste0(Temperature, Precipitation, Humidity))%>%
  group_by(ClimMeasured)%>%
    summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 
  
```

**Proportion of each disease group in dataset**

```{r studycounts1}

df%>%
  filter(General_Disease != "Multiple") %>%
  group_by(General_Disease) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N))

```

**Proportion of specific diseases in dataset**

```{r studycounts2}

df%>%
  filter(General_Disease != "Multiple") %>%
  group_by(Disease, General_Disease) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N))

```

**Response variables examined**

```{r responsecounts}

df%>%
  group_by(General_response) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Host Taxonomic Groups**

```{r taxgroups}
df%>%
  group_by(Principal_reservoir) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Type of Pathogen**

```{r pathtype}

df%>%
  group_by(Pathogen) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 
```

**Linear vs Non-linear**

```{r lnl}
df%>%
  group_by(Linear_Nonlinear) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Statistical Method**

```{r statsmeth2}
df%>%
  group_by(General_Stats_Method) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Statistical Method**

```{r statsmeth}
df%>%
  group_by(Statistical_method) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

# Results Section 2: Reported significance of climate measures in studies and publication bias

```{r repsignificance}

#filter out records without P values
tmp = df %>% filter(P.value_general!="" & !is.na(P.value_general))

#unify P-values
tmp$P.value_general[tmp$P.value_general=="<.0.05" |
                    tmp$P.value_general=="<0..05"] = "<0.05"
tmp = as.data.frame(table(tmp$P.value_general)); names(tmp) = c("significance","freq")

#calculate proportions
tmp %<>% mutate(P = freq/sum(freq))

#Proportion of studies significant at different levels
cat("Proportion of studies significant at alfa = 0.05: ", tmp %>% filter(significance!=">0.05") %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum), "\n")

cat("Proportion of studies significant at alfa = 0.01: ", tmp %>% filter(!significance %in% c("<0.05",">0.05")) %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum),"\n")

cat("Proportion of studies significant at alfa = 0.001: ", tmp %>% filter(!significance %in% c("<0.05",">0.05","<0.01")) %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum),"\n")

```

### Chi-square direction reported + groups removed

```{r chisq_groups_removed_function}

library(dplyr)

# Function to perform chi-square tests on sampled data
perform_chi_tests <- function(data, n_samples, percentage) {
  results <- data.frame(i = 1:n_samples)

  for (i in 1:n_samples) {
    #sample % of data
    smp <- data[sample(1:nrow(data), size = round(nrow(data) * percentage), replace = TRUE),]
    smp$Direction = as.factor(smp$Direction)
    #perform chi-tests
    chi_test_all <- chisq.test(xtabs(data = smp, Dummy ~ Direction))
    chi_test_hum <- chisq.test(xtabs(data = smp[smp$Environmental_condition == "Humidity",], Dummy ~ Direction))
    chi_test_temp <- chisq.test(xtabs(data = smp[smp$Environmental_condition == "Temperature",], Dummy ~ Direction))
    chi_test_prec <- chisq.test(xtabs(data = smp[smp$Environmental_condition == "Precipitation",], Dummy ~ Direction))
    
    #extract statistics and p-values
    results$chi_all[i] <- chi_test_all$statistic
    results$p_all[i] <- chi_test_all$p.value
    
    results$hum_chi[i] <- chi_test_hum$statistic
    results$hum_p[i] <- chi_test_hum$p.value
    
    results$temp_chi[i] <- chi_test_temp$statistic
    results$temp_p[i] <- chi_test_temp$p.value
    
    results$prec_chi[i] <- chi_test_prec$statistic
    results$prec_p[i] <- chi_test_prec$p.value
  }
  
  return(results)
}

# Function to calculate summary statistics (P-values)
calculate_means <- function(results) {
  means <- c(
    p_all = mean(results$p_all, na.rm = TRUE),
    chi_all = mean(results$chi_all, na.rm = TRUE),
    chi_ci_lower_all = t.test(results$chi_all, conf.level = 0.95)$conf.int[1], #Chi Ci Lower
    chi_ci_upper_all = t.test(results$chi_all, conf.level = 0.95)$conf.int[2], #Chi Ci Upper
    hum_p = mean(results$hum_p, na.rm = TRUE),
    hum_chi = mean(results$hum_chi, na.rm = TRUE),
    chi_ci_lower_hum = t.test(results$hum_chi, conf.level = 0.95)$conf.int[1], #Chi Ci Lower
    chi_ci_upper_hum = t.test(results$hum_chi, conf.level = 0.95)$conf.int[2], #Chi Ci Upper
    temp_p = mean(results$temp_p, na.rm = TRUE),
    temp_chi = mean(results$temp_chi, na.rm = TRUE),
    chi_ci_lower_temp = t.test(results$temp_chi, conf.level = 0.95)$conf.int[1], #Chi Ci Lower
    chi_ci_upper_temp = t.test(results$temp_chi, conf.level = 0.95)$conf.int[2], #Chi Ci Upper
    prec_p = mean(results$prec_p, na.rm = TRUE),
    prec_chi = mean(results$prec_chi, na.rm = TRUE),
    chi_ci_lower_prec = t.test(results$prec_chi, conf.level = 0.95)$conf.int[1], #Chi Ci Lower
    chi_ci_upper_prec = t.test(results$prec_chi, conf.level = 0.95)$conf.int[2] #Chi Ci Upper
  )
  
  return(means)
}

# Main analysis function (removing 1 group from the 3 biggest groups at a time)
chisq_remove_groups <- function(df, grouping_factor, percentage = percentage, n_samples = 1000) {
  
  # none_removed doesnt remove any groups and performs analysis for full dataset
  # and 3 climatic subsets
  if(grouping_factor!="none_removed"){
      group_summary <- df %>%
        group_by(across(all_of(grouping_factor))) %>%
        summarise(N = n_distinct(Reference_ID)) %>%
        mutate(P = N / sum(N)) %>%
        arrange(desc(N))
      #identify top 3 groups per count
        top_groups <- as.vector(unlist(group_summary[1:3, 1]))
        top_groups = top_groups[!is.na(top_groups)]}
  
  means_df <- list()
  
  if(grouping_factor!="none_removed"){
    for (group_rm in top_groups) {
      tmp <- df %>%
        filter(Direction != "") %>%
        filter(P.value_general != "") %>%
        filter(P.value_general != ">0.05") %>%
        filter(across(all_of(grouping_factor)) != group_rm)
      #check if all env. categories have mroe than 10 records
      if (all(table(tmp$Environmental_condition) > 10)) {
        results <- perform_chi_tests(tmp, n_samples, percentage)
        means <- calculate_means(results)
        means_df[[group_rm]] <- means
        }
      }
} else {
   tmp <- df %>%
      filter(Direction != "") %>%
      filter(P.value_general != "") %>%
      filter(P.value_general != ">0.05")
   
   if (all(table(tmp$Environmental_condition) >= 10)) {
      results <- perform_chi_tests(tmp, n_samples, percentage)
      means <- calculate_means(results)
      means_df[[grouping_factor]] <- means
    }
  }
  
  if(grouping_factor!="none_removed"){
    means_chi = lapply(means_df, function(sublist) { sublist[grep("chi", names(sublist))]})
    means_pval = lapply(means_df, function(sublist) { sublist[!grepl("chi", names(sublist))]})} 
  else {
    means_chi = list()
    means_chi[[grouping_factor]] = means_df[[grouping_factor]][grep("chi",names(means_df[[grouping_factor]]))]
    means_pval = list()
    means_pval[[grouping_factor]] = means_df[[grouping_factor]][!grepl("chi",names(means_df[[grouping_factor]]))]
  }
  
  #Transform P-values to simplified significance levels
  levels_means_pval <- as.data.frame(lapply(means_pval, function(x) {
    ifelse(x < 0.01, "**", ifelse(x < 0.05, "*", ">0.05"))
  }))
  means_pval = as.data.frame(means_pval)
  rownames(levels_means_pval) <- rownames(means_pval)
  means_chi = as.data.frame(means_chi)
  
  return(list(means_pval = means_pval, levels_means_pval = levels_means_pval,
              means_chi = means_chi))
  
}
```

### Groups removed one at time

```{r full_chisq_and_groups_removed}
#Set the percentage of data to use
percentage = 0.8
#Run the chi-square tests on full dataset
none_removed = chisq_remove_groups(df, grouping_factor = "none_removed",percentage = percentage, n_samples = 1000)
#Run the chi-square tests on the dataset with 1 of 3 top categories from the grouping factor removed at a time
disease_removed = chisq_remove_groups(df, grouping_factor = "General_Disease", percentage = percentage, n_samples = 1000)
country_removed = chisq_remove_groups(df, grouping_factor = "Country", percentage = percentage, n_samples = 1000)
princ_reservoir_removed = chisq_remove_groups(df, grouping_factor = "Principal_reservoir", percentage = percentage, n_samples = 1000)
stat_method_removed = chisq_remove_groups(df, grouping_factor = "Statistical_method", percentage = percentage, n_samples = 1000)
pathogen_group_removed = chisq_remove_groups(df, grouping_factor = "Pathogen", percentage = percentage, n_samples = 1000)
transmission_type_removed = chisq_remove_groups(df, grouping_factor = "Transmission_type", percentage = percentage, n_samples = 1000)

combined_pvals = (cbind(none_removed$means_pval, disease_removed$means_pval,
                        country_removed$means_pval, princ_reservoir_removed$means_pval, 
                        pathogen_group_removed$means_pval,
                        stat_method_removed$means_pval, transmission_type_removed$means_pval))

combined_levels = (cbind(none_removed$levels_means_pval, disease_removed$levels_means_pval,
                        country_removed$levels_means_pval, princ_reservoir_removed$levels_means_pval, 
                        pathogen_group_removed$levels_means_pval,
                        stat_method_removed$levels_means_pval, transmission_type_removed$levels_means_pval))

combined_chi = (cbind(none_removed$means_chi, disease_removed$means_chi,
                        country_removed$means_chi, princ_reservoir_removed$means_chi, 
                      pathogen_group_removed$means_chi,
                        stat_method_removed$means_chi, transmission_type_removed$means_chi))

#Transform Chi values ----------------------
chi_transformed <- combined_chi %>%
  rownames_to_column(var = "Metric") %>%
  pivot_longer(cols = -Metric, names_to = "Group", values_to = "Value")

chi_wide <- chi_transformed %>% # Convert to wide format with diseases as rows and metrics as columns
  pivot_wider(names_from = Metric, values_from = Value)

#Transform P values ----------------------
pval_transformed <- combined_pvals %>%
  rownames_to_column(var = "Metric") %>%
  pivot_longer(cols = -Metric, names_to = "Group", values_to = "Value")

# Convert to wide format with diseases as rows and metrics as columns
pvals_wide <- pval_transformed %>%
  pivot_wider(names_from = Metric, values_from = Value)

#Transform P levels ----------------------
pval_levels_transformed <- combined_levels %>%
  rownames_to_column(var = "Metric") %>%
  pivot_longer(cols = -Metric, names_to = "Group", values_to = "Value")
# Convert to wide format with diseases as rows and metrics as columns
pvals_levels_wide <- pval_levels_transformed %>%
  pivot_wider(names_from = Metric, values_from = Value)
names(pvals_levels_wide) = c("Group","P_level_all", "P_level_hum", "P_level_temp", "P_level_prec")

#Join dataframes
full_results <- chi_wide %>%
  left_join(pvals_wide, by = "Group") %>%
  left_join(pvals_levels_wide, by = "Group")

# Split the data frame into four separate data frames based on column names
df_all <- full_results %>% select(Group, contains("all"))
df_hum <- full_results %>% select(Group, contains("hum"))
df_temp <- full_results %>% select(Group, contains("temp"))
df_prec <- full_results %>% select(Group, contains("prec"))


# Function to rename columns in each dataframe
rename_columns <- function(df) {
  df %>%
    rename_with(~ str_replace(., "chi_all|prec_chi|temp_chi|hum_chi", "Chi_Square")) %>%
    rename_with(~ str_replace(., "chi_ci_lower_.*", "Chi_Lower_CI")) %>%
    rename_with(~ str_replace(., "chi_ci_upper_.*", "Chi_Upper_CI")) %>% 
    rename_with(~ str_replace(., "p_all|temp_p|prec_p|hum_p", "P_value"))
}

# Apply the renaming function to each dataframe
df_all <- rename_columns(df_all)
df_hum <- rename_columns(df_hum)
df_temp <- rename_columns(df_temp)
df_prec <- rename_columns(df_prec)

# Combine the data frames into a list
list_of_dfs <- list(Overall = df_all, Humidity = df_hum, Temperature = df_temp, Precipitation = df_prec)
print(list_of_dfs)

# # Define the file name
# file_name <- "chisq_results.xlsx"
# library(openxlsx)
# # Create a new workbook
# wb <- createWorkbook()
# # Loop through each dataframe in the list and add it to a new sheet
# for (name in names(list_of_dfs)) {
#   addWorksheet(wb, name)
#   writeData(wb, name, list_of_dfs[[name]])
# }
# # Save the workbook
# saveWorkbook(wb, file_name, overwrite = TRUE)
```

### Secondary reporting rates

```{r secondary_reporting_rates}
#SETUP ---------------------------------------------------------------------
second = df

#filter out records without P values
second = second %>% filter(P.value_general!="" & !is.na(P.value_general))

#unify P-values
second$P.value_general[second$P.value_general=="<.0.05" |
                       second$P.value_general=="<0..05" | 
                       second$P.value_general=="<0.0001"|
                       second$P.value_general=="<0.001" | 
                       second$P.value_general=="<0.01"] = "<0.05"
#save IDs of unique studies
studies = unique(second$Reference_ID)

#make dataframe to write in
studies_with_secondary = data.frame(studies = studies, secondary = NA, any_significant = NA,
                                    two_or_more_significant = NA)

#EVALUATE REPORTING RATES ------------------------------------------------------------------------
for(i in (studies_with_secondary$studies)){
 sub = second %>% filter(Reference_ID==i) #subset records for 1 study
 levels = sub$P.value_general #list significance levels within the study
 
 #note studies with 2 or more significance values reported
 if(length(levels)>1){
   studies_with_secondary$secondary[studies_with_secondary$studies==i] = TRUE
 }
 #note studies with at least 1 significant result
 if("<0.05" %in% levels){
    studies_with_secondary$any_significant[studies_with_secondary$studies==i] = TRUE
 } else{
   studies_with_secondary$any_significant[studies_with_secondary$studies==i] = FALSE
 }
 #note studies with more than 1 significant result
 if(sum(levels=="<0.05")>1){
   studies_with_secondary$two_or_more_significant[studies_with_secondary$studies==i] = TRUE
 } else {studies_with_secondary$two_or_more_significant[studies_with_secondary$studies==i] = FALSE}
}

# FILTER STUDIES WITH >1 REPORTED SIGNIFICANCE LEVEL & >1 SIGNIFICANT RESULT (or >=1 SIGNIFICANT?)-------
tmp = studies_with_secondary %>% filter(secondary==TRUE & any_significant==TRUE)
#tmp = studies_with_secondary %>% filter(secondary==TRUE & two_or_more_significant==TRUE) #can choose this one 

tmp2 = df %>% filter(Reference_ID %in% tmp$studies) %>% 
  filter(P.value_general!="" & !is.na(P.value_general))

#unify P-values
tmp2$P.value_general[tmp2$P.value_general=="<.0.05" |
                    tmp2$P.value_general=="<0..05"] = "<0.05"
tmp2 = as.data.frame(table(tmp2$P.value_general)); names(tmp2) = c("significance","freq")

#calculate proportions
tmp2 %<>% mutate(P = freq/sum(freq))

#SIGNIFICANCE AT DIFFERENT LEVELS ------------------------------------------------
cat("Proportion of studies significant at alfa = 0.05: ", 
    tmp2 %>% filter(significance!=">0.05") %>% 
    summarise(total_sum = sum(P, na.rm = TRUE)) %>% 
    pull(total_sum), "\n")

cat("Proportion of studies significant at alfa = 0.01: ",
    tmp2 %>% filter(!significance %in% c("<0.05",">0.05")) %>%
    summarise(total_sum = sum(P, na.rm = TRUE)) %>% 
    pull(total_sum),"\n")

cat("Proportion of studies significant at alfa = 0.001: ",
    tmp2 %>% filter(!significance %in% c("<0.05",">0.05","<0.01")) %>%
      summarise(total_sum = sum(P, na.rm = TRUE)) %>%
      pull(total_sum),"\n")

```

### Journals IF vs Effect Sizes/Pvals

```{r journals, echo=TRUE, message=TRUE, warning=TRUE, fig.show='hold',fig.width = 10, fig.height = 4}
#-----ES.   ------------------------------------------------------------------
library(patchwork)
library(ggplot2)
library(dplyr)

# Data filtering and preparation
pub1 = df %>% filter(!is.na(es) & !is.na(Journal_5yr_Impact))
pub1 %<>% dplyr::select(es, Journal_5yr_Impact, Environmental_condition)
dat_pub1 = pub1
dat_pub1$es = abs(dat_pub1$es)

# Scatter plot with regression line
p1 <- ggplot(dat_pub1, aes(x = Journal_5yr_Impact, y = es)) +
  geom_point(color = "forestgreen", alpha = 0.5) +
  geom_smooth(method = "lm", color = "black", se = TRUE, alpha = 0.5) +
  ggtitle("Effect Size vs Journal Impact Factor") +
  xlab("Journal 5-year Impact Factor") +
  ylab("Effect Size (Hedge's g)") +
  theme_minimal()

# Calculate Pearson correlation coefficient
correlation1 <- cor.test(dat_pub1$es, dat_pub1$Journal_5yr_Impact)

# Annotate the scatter plot with the correlation coefficient
p1 <- p1 + annotate("text", x = max(dat_pub1$Journal_5yr_Impact) * 0.6, y = max(dat_pub1$es) * 0.8,
                    label = paste("Pearson r:", round(correlation1$estimate, 2),
                                  "\nP-value:", format(correlation1$p.value, digits = 3)),
                    color = "black", size = 5, hjust = 0)
#------------------------------------------------------------------------------

# Data filtering and preparation for the second plot
pub2 = df %>% filter(!is.na(P.value_specific) & !is.na(Journal_5yr_Impact))
pub2 %<>% dplyr::select(P.value_specific, Journal_5yr_Impact, Environmental_condition)
dat_pub2 = pub2
dat_pub2$P.value_specific = abs(dat_pub2$P.value_specific)

# Scatter plot with regression line
p2 <- ggplot(dat_pub2, aes(x = Journal_5yr_Impact, y = P.value_specific)) +
  geom_point(color = "cornflowerblue", alpha = 0.5) +
  geom_smooth(method = "lm", color = "black", se = TRUE) +
  ggtitle("Reported P-value vs Journal Impact Factor") +
  xlab("Journal 5-year Impact Factor") +
  ylab("P-value reported") +
  theme_minimal()

# Calculate Pearson correlation coefficient
correlation2 <- cor.test(dat_pub2$Journal_5yr_Impact, dat_pub2$P.value_specific)

# Annotate the scatter plot with the correlation coefficient
p2 <- p2 + annotate("text", x = max(dat_pub2$Journal_5yr_Impact) * 0.6, y = max(dat_pub2$P.value_specific) * 0.8,
                    label = paste("Pearson r:", round(correlation2$estimate, 2),
                                  "\nP-value:", format(correlation2$p.value, digits = 3)),
                    color = "black", size = 5, hjust = 0)

ggarrange(p1, p2, ncol = 2, nrow = 1)
```

### Funnel plots of SE vs ES

```{r funnelplots, echo=FALSE, fig.height=5, fig.show='hold', fig.width=8.5, include=TRUE, fig.cap='Fig. S3: Funnel plots of standard errors against effect sizes'}
# Define the function to create the funnel plot
create_funnel_plot <- function(tmp, point_color, title_plot) {
  
  # Calculate the mean effect size
  mean_es <- mean(tmp$es, na.rm = TRUE)
  
  # Calculate the limits for the funnel regions
  max_se <- max(tmp$se, na.rm = TRUE)
  
  # Calculate the x-axis limits based on the maximum absolute value of effect size
  max_abs_es <- max(abs(tmp$es), na.rm = TRUE)
  xlim_range <- c(-max_abs_es, max_abs_es)
  
  # Create the funnel plot 
  funnel_plot <- ggplot(tmp, aes(x = es, y = se)) +
    geom_polygon(data = data.frame(
      x = c(mean_es - 1.96 * max_se, mean_es + 1.96 * max_se, mean_es),
      y = c(max_se, max_se, 0)
    ), aes(x = x, y = y), fill = point_color, alpha = 0.1) +  # Add 95% confidence region
    geom_point(shape = 1, color = point_color) +  # Scatter plot of effect sizes vs. standard errors
    geom_vline(xintercept = mean_es, linetype = "dashed") +  # Vertical line at overall effect size
    scale_x_continuous(limits = xlim_range) +  # Set x-axis limits to ensure zero is centered
    scale_y_reverse() +  # Reverse the y-axis to have standard errors decreasing upwards
    xlab("Effect Size (Hedge's g)") +
    ylab("Standard Error") +
    ggtitle(title_plot) +
    theme_minimal()

  return(funnel_plot)
}

f1 <- df %>% filter(!is.na(se) & !is.na(es))
funnel_plot_1 <- create_funnel_plot(f1, "forestgreen", "All env. measures")

f2 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Temperature")
funnel_plot_2 <- create_funnel_plot(f2, "cyan4", "Temperature")

f3 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Precipitation")
funnel_plot_3 <- create_funnel_plot(f3, "cyan4", "Precipitation")

f4 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Humidity")
funnel_plot_4 <- create_funnel_plot(f4, "cyan4", "Humidity")

comb = ggarrange(funnel_plot_1, funnel_plot_2, funnel_plot_3, funnel_plot_4, nrow = 2, ncol = 2)
print(comb)
```

### Eggers test

```{r eggers_tests}

regtest_all = regtest(f1$es, sei=f1$se)
regtest_temp = regtest(f2$es, sei=f2$se)
regtest_prec = regtest(f3$es, sei=f3$se)
regtest_hum = regtest(f4$es, sei=f4$se)

list_regtest = list(All = regtest_all$pval, Temperature = regtest_temp$pval, Precipitation = regtest_prec$pval, Humidity = regtest_hum$pval)

print("P-values of regtests:")
print(list_regtest)

#--------------------------------------
# Extract the p-values and z-values
pval_all <- regtest_all$pval
zval_all <- regtest_all$zval
est_all <- regtest_all$est

pval_temp <- regtest_temp$pval
zval_temp <- regtest_temp$zval
est_temp <- regtest_temp$est

pval_prec <- regtest_prec$pval
zval_prec <- regtest_prec$zval
est_prec <- regtest_prec$est

pval_hum <- regtest_hum$pval
zval_hum <- regtest_hum$zval
est_hum <- regtest_hum$est

# Create a data frame
results_regtests <- data.frame(
  Data = c("Overall_dataset", "Temperature", "Precipitation", "Humidity"),
  Intercept = c(est_all, est_temp, est_prec, est_hum),
  Z_value = c(zval_all, zval_temp, zval_prec, zval_hum),
  P_value = c(pval_all, pval_temp, pval_prec, pval_hum)
)

write.csv(results_regtests, 'outputs/tables/eggers_funnel_tests.csv', row.names = F)
```

### P-value distribution

```{r}
print(unique(df$P.value_general))

#P-VALUES GENERAL--------------------------------------------
#filter out records without P values
tmp = df %>% filter(P.value_general!="")

#unify P-values
tmp$P.value_general[tmp$P.value_general=="<.0.05" |
                    tmp$P.value_general=="<0..05"] = "<0.05"

barplot(prop.table(table(tmp$P.value_general)))

#P-VALUP.value_specific SPECIFIC-----------------------------

tmp %<>% filter(!is.na(P.value_specific))

# Plot the frequency of P.value_specific
## HISTOGRAM
ggplot(data = data.frame(p_value = tmp$P.value_specific), aes(x = p_value)) +
  geom_histogram(binwidth = 0.005, fill = "skyblue", color = "black") +
  labs(x = "P-value", y = "Frequency", title = "Frequency of P.value_specific") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +  # Add vertical line at x = 0.05
  theme_minimal()

## DENSITY PLOT
ggplot(data = data.frame(p_value = tmp$P.value_specific), aes(x = p_value)) +
  geom_density(fill = "skyblue", color = "black") +
  labs(x = "P-value", y = "Density", title = "Density Plot of P.value_specific") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +  # Add vertical line at x = 0.05+
  theme_minimal()

# Plot the density of P.value_specific focusing on the area around 0.05
tmp = df %>% filter(!is.na(P.value_specific)) %>% filter(P.value_specific<=0.10)
# DENSITY PLOT
ggplot(data = data.frame(p_value = tmp$P.value_specific), aes(x = p_value)) +
  geom_density(fill = "skyblue", color = "black") +
  labs(x = "P-value", y = "Density", title = "Density Plot of P.value_specific (Zoomed In)") +
  xlim(0, 0.1) +  # Zoom in on the area around 0.05
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +  # Add vertical line at x = 0.05
  theme_minimal()
```

### P-CURVE

Using <https://rdrr.io/github/MathiasHarrer/dmetar/man/pcurve.html>.

```{r PCURVE}
#Using <https://rdrr.io/github/MathiasHarrer/dmetar/man/pcurve.html>.
if (!require(dmetar, quietly = TRUE)) {
  install.packages("dmetar")
  library(dmetar)
}

tmp = df %>% filter(!is.na(se) & !is.na(es) & !is.na(Reference_ID)) %>% filter(P.value_general!=">0.05" & !is.na(P.value_general))
tmp %<>% dplyr::select(.,se, es, Reference_ID, es)

names(tmp) = c("seTE", "TE","studlab")
pcurve_results = pcurve(tmp, effect.estimation = FALSE)

#Right skewness test for Full curve & Half curve has p = 0 (p<0.001)
#Which suggests no P-hacking
```

### Journal IF vs Effect Size

```{r journal_vs_pval}
library(patchwork)
tmp = df %>% filter(!is.na(es) & !is.na(Journal_5yr_Impact))
tmp %<>% dplyr::select(es, Journal_5yr_Impact, Environmental_condition)
#tmp %<>% filter(Environmental_condition=="Precipitation")
data=tmp

data$es = abs(data$es)

# Histogram for es
p1 <- ggplot(data, aes(x = es)) +
  geom_histogram(bins = 30, color = "black", fill = "blue", alpha = 0.5) +
  #geom_density(color = "red", size = 1) +
  ggtitle("Distribution of es") +
  xlab("es") +
  theme_minimal()

# Histogram for Journal_5yr_Impact
p2 <- ggplot(data, aes(x = Journal_5yr_Impact)) +
  geom_histogram(bins = 30, color = "black", fill = "forestgreen", alpha = 0.7) +
  #geom_density(color = "red", size = 1) +
  ggtitle("Distribution of Journal_5yr_Impact") +
  xlab("Journal_5yr_Impact") +
  theme_minimal()

# Scatter plot with regression line
p3 <- ggplot(data, aes(x = es, y = Journal_5yr_Impact)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("es vs Journal_5yr_Impact") +
  xlab("es") +
  ylab("Journal_5yr_Impact") +
  theme_minimal()

# Calculate Pearson correlation coefficient
correlation <- cor.test(data$es, data$Journal_5yr_Impact)

# Annotate the scatter plot with the correlation coefficient
p3 <- p3 + annotate("text", x = 0.5, y = max(data$Journal_5yr_Impact), 
                    label = paste("Pearson r:", round(correlation$estimate, 2),
                                  "\nP-value:", format(correlation$p.value)),
                    color = "black", size = 5, hjust = 0)


# Improved Boxplot with Jitter for es by Journal_5yr_Impact
p4 <- ggplot(data, aes(x = as.factor(round((Journal_5yr_Impact))), y = es)) +
  geom_boxplot(fill = "orange", color = "black", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5, color = "blue") +
  ggtitle("Boxplot and Jitter of es by Journal_5yr_Impact") +
  xlab("Journal_5yr_Impact") +
  ylab("es") +
  theme_minimal()

# Arrange all plots in a grid
ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

```

### Journal IF vs P-value

```{r journal_vs_es}

tmp = df %>% filter(!is.na(P.value_specific) & !is.na(Journal_5yr_Impact))
tmp %<>% dplyr::select(P.value_specific, Journal_5yr_Impact, Environmental_condition)
#tmp %<>% filter(Environmental_condition=="Temperature")
data = tmp; data$P.value_specific = abs(data$P.value_specific)

# Histogram for P.value_specific
p1 <- ggplot(data, aes(x = P.value_specific)) +
  geom_histogram(bins = 30, color = "black", fill = "blue", alpha = 0.5) +
  #geom_density(color = "red", size = 1) +
  ggtitle("Distribution of P.value_specific") +
  xlab("P.value_specific") +
  theme_minimal()

# Histogram for Journal_5yr_Impact
p2 <- ggplot(data, aes(x = Journal_5yr_Impact)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "forestgreen", alpha = 0.7) +
  #geom_density(color = "red", size = 1) +
  ggtitle("Distribution of Journal_5yr_Impact") +
  xlab("Journal_5yr_Impact") +
  theme_minimal()

# Scatter plot with regression line
p3 <- ggplot(data, aes(x = Journal_5yr_Impact, y = P.value_specific)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("P.value_specific vs Journal_5yr_Impact") +
  xlab("Journal_5yr_Impact") +
  ylab("P.value_specific") +
  theme_minimal()

# Calculate Pearson correlation coefficient
correlation <- cor.test(data$Journal_5yr_Impact, data$P.value_specific)

# Annotate the scatter plot with the correlation coefficient
p3 <- p3 + annotate("text", x = 0.5, y = max(data$P.value_specific), 
                    label = paste("Pearson r:", round(correlation$estimate, 2),
                                  "\nP-value:", format(correlation$p.value)),
                    color = "black", size = 5, hjust = 0)


# Improved Boxplot with Jitter for P.value_specific by Journal_5yr_Impact
p4 <- ggplot(data, aes(x = as.factor(round((Journal_5yr_Impact))), y = P.value_specific)) +
  geom_boxplot(fill = "orange", color = "black", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.3, color = "blue") +
  ggtitle("Boxplot and Jitter of P.value_specific by Journal_5yr_Impact") +
  xlab("Journal_5yr_Impact") +
  ylab("P.value_specific") +
  theme_minimal()

# Arrange all plots in a grid
ggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2)

```

# Results Section 3: Strength of climate effects of zoonotic diseases

**Effect Size Category Grouping**

```{r numpergroup}

# group effect sizes
df = df%>%
  mutate(es_cat = ifelse(es < -0.2, 'Negative effect (g < -0.2)', #if CI contains 0
                         ifelse(abs(es) < 0.2, 'No effect', # otherwise negative
                                'Positive effect (g > 0.2)')) # or positive
  )

df%>%#
  subset(Linear_Nonlinear == 'Linear')%>%
  count(es_cat, Environmental_condition)%>%
  group_by(Environmental_condition)%>%
  mutate(prop = n /sum(n), tot = sum(n))
```

By category:

```{r}
df%>%#
  subset(Linear_Nonlinear == 'Linear')%>%
  count(es_cat, Environmental_condition)%>%
  group_by(Environmental_condition)%>%
  mutate(prop = n /sum(n), tot = sum(n))
```

## Determining how category distributions differ from full dataset

## Defining statistical test

```{r}
ad2_stat <- function(x, y) {

  # Sample sizes
  n <- length(x)
  m <- length(y)

  # Pooled sample and pooled ecdf
  z <- c(x, y)
  z <- z[-which.max(z)] # Exclude the largest point
  H <- rank(z) / (n + m)

  # Statistic computation via ecdf()
  (n * m / (n + m)^2) * sum((ecdf(x)(z) - ecdf(y)(z))^2 / ((1 - H) * H))

}

```

```{r}
# A homogeneity test using the Anderson-Darling statistic
perm_comp_test <- function(x, y, B = 1e3) {

  # Sizes of x and y
  n <- length(x)
  m <- length(y)

  # Test statistic function. Requires TWO arguments, one being the original
  # data (X_1, ..., X_n, Y_1, ..., Y_m) and the other containing the random
  # index for permuting the sample
  Tn <- function(data, perm_index) {

    # Permute sample by perm_index
    data <- data[perm_index]

    # Split into two samples
    x <- data[1:n]
    y <- data[(n + 1):(n + m)]

    # Test statistic -- MODIFY DEPENDING ON THE PROBLEM
    ad2_stat(x = x, y = y)

  }

  # Perform permutation resampling with the aid of boot::boot
  Tn_star <- boot::boot(data = c(x, y), statistic = Tn,
                        sim = "permutation", R = B)

  # Test information -- MODIFY DEPENDING ON THE PROBLEM
  method <- "Permutation-based Anderson-Darling test of homogeneity"
  alternative <- "any alternative to homogeneity"

  # p-value: modify if rejection does not happen for large values of the
  # test statistic
  pvalue <- mean(Tn_star$t > Tn_star$t0)

  # Construct an "htest" result
  result <- list(statistic = c("stat" = Tn_star$t0), p.value = pvalue,
                 statistic_perm = drop(Tn_star$t),
                 B = B, alternative = alternative, method = method,
                 data.name = deparse(substitute(x)))
  class(result) <- "htest"

  # Return "htest"
  return(result)

}

```

Defining categories and parameters to use in procedure.

```{r catdefine}
categories = list(Transmission_type = c('Vectored', 'Non-vectored'), 
                  Principal_reservoir = c('Rodents', 'Mammals (multispecies)', 'Livestock'), 
                  Pathogen = c('Virus', 'Bacteria'), 
                  vector = c('Mosquito', 'Tick', 'Mite')
                  )

# random number seed
set.seed(42)
```

**Anderson Darling Test: without subset removal**

```{r ad1}

# create named list for results
pvals  = vector("list")
test_stats = vector('list')

for(condition in unique(df$Environmental_condition)){
  # separate out climate factor
  temp = df%>%
    subset(Linear_Nonlinear == 'Linear')%>%
    subset(Environmental_condition == condition)
  
  for(category in names(categories)){
    for(group in categories[[category]]){
      # separate out category  
      group_df = temp%>%
        filter(.data[[category]] == group)
      
      ad = perm_comp_test(temp$es, group_df$es)

      pvals[[paste(condition, category, group, sep='.')]] = ad$p.value
      test_stats[[paste(condition, category, group, sep='.')]] = ad$statistic
    }
  }
}
```

**Create Dataframe**

```{r}
results_df = data.frame(id = names(pvals),
                        p_val = unlist(pvals),
                        stat = unlist(test_stats))

```

**Anderson-Darling Test with subset removal**

```{r ad2}

# create named list for results
pvals  = vector("list")
test_stats = vector('list')

for(condition in unique(df$Environmental_condition)){
  # separate out climate factor
  temp = df%>%
    subset(Linear_Nonlinear == 'Linear')%>%
    subset(Environmental_condition == condition)
  
  for(category in names(categories)){
    for(group in categories[[category]]){
      # separate out category  
      group_df = temp%>%
        filter(.data[[category]] == group)
      
      # filter out group for random sample
      temp_filtered = temp%>%
        filter(.data[[category]] != group)
      
      ad = perm_comp_test(temp_filtered$es, group_df$es)
      
      #ad = ad2_stat(nonvec$es, vec$es)

      pvals[[paste(condition, category, group, sep='.')]] = ad$p.value
      test_stats[[paste(condition, category, group, sep='.')]] = ad$statistic
      #pvals[[paste(condition, category, group, sep='.')]] = 1- goftest::pAD(q=ad)
      #test_stats[[paste(condition, category, group, sep='.')]] = ad
    }
  }
}
```

**Combine and save dataframes**

```{r}
results_df2 = data.frame(id = names(pvals),
                        p_val_removed = unlist(pvals),
                        stat_removed = unlist(test_stats))

results_df = left_join(results_df, results_df2)

results_df = results_df%>%
  separate_wider_delim(cols = id, delim = ".", names = c("Environmental_condition", "Category", "Group"))

rownames(results_df) = NULL

results_df 

write.csv(results_df, 'outputs/tables/DistributionADTest_Results.csv', row.names = F)
```

## 3.2 Reported direction vs calculated

```{r rep_vs_calc}
# Filter only P-val <0.05 & no missing directions/es_cats
tmp = df  %>% 
       filter(!is.na(Direction) & !is.na(es_cat) & Direction!="") %>% 
       filter(P.value_general != "" & P.value_general != ">0.05") %>%
       select(Direction, es, es_cat, P.value_general)


```

## Results Section: Comparison of Vector and Non-vector borne disease

```{r}

df%>%
  subset(Linear_Nonlinear == 'Linear')%>%
  group_by(Transmission_type, Environmental_condition)%>%
  summarize(mn = mean(es), sd = sd(es), n=n())

```

Testing vectored vs non-vectored

```{r ad2}

# create named list for results
pvals  = vector("list")
test_stats = vector('list')

for(condition in unique(df$Environmental_condition)){
  # separate out climate factor
  vec = df%>%
    subset(Linear_Nonlinear == 'Linear')%>%
    subset(Environmental_condition == condition)%>%
    subset(Transmission_type == 'Vectored')
  
  nonvec = df%>%
    subset(Linear_Nonlinear == 'Linear')%>%
    subset(Environmental_condition == condition)%>%
    subset(Transmission_type == 'Non-vectored')
  
      
  #ad = ad_test(vec$es, nonvec$es, nboots=10000)
  ks = ks.test(nonvec$es, vec$es, alternative = 'greater')
  pvals[[condition]] = ks$p.value
  test_stats[[condition]] = ks$statistic
}
```

# Main figures

## Figure 1

```{r fig1, echo=TRUE, fig.height=9.5, fig.width=10.5}
source('figure1.R')

plot(Fig1)

#ggsave(Fig1, file="outputs/Figure1.jpg", device="jpg", units="in", width=10.5, height=9.5, dpi=600, scale=0.93)
```

## Figure 2

```{r fig2, include=FALSE}
source('figure2.R')
#colors = c("#053C5E" , '#fee8c8', '#A31621')
#colors = c( "#29A1AB" , '#fee8c8', '#AB3329')
colors = c( "#00A8A5" , '#fee8c8', '#A80003')

f2 = createFig2(df, results_df, colors)

ggsave(f2, file="outputs/Figure2.png", device="png", units="in", width=12, height=9, dpi=600, scale=0.92)
```

## Figure 3

```{r fig3, echo=FALSE}
source('Figure3.R')
colors = c('#A80003',"#00A8A5")
f3 = createFig3(df, colors)

ggsave(f3, file="outputs/Figure3.png", device="png", units="in", width=8, height=6, dpi=300, scale=1.25)
```
