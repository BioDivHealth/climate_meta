---
title: "Climate impacts on zoonotic disease meta-analysis results"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='asis'}
# deps
library(tidyr)
#library(twosamples)
library(here)
library(magrittr)
library(tidyverse)
library(jsonlite)

knitr::opts_chunk$set(echo = FALSE)

# load dataset
df = read.csv(here::here("data","dataset_final_g.csv"))
df = unique(df) #ensure all rows are unique

# change vector names
#df = df%>%
#  mutate(Transmission_type = ifelse(Transmission_type == 'HVH', 'Vectored', 'Non-vectored'))

# add parasite group
# df = df%>%
#   mutate(Pathogen = ifelse(Pathogen == 'P', 'Parasite',
#                                 ifelse(Pathogen == 'V', 'Virus',
#                                         'Bacteria')))

cat(paste('Number of unique studies:', length(unique(df$Reference_ID))))
cat(paste('Number of pathogens:', length(unique(df$Disease))))
cat(paste('Number of countries:', length(unique(df$Country))))

sort(table(df$Country))
```

# Results Section 1: Literature search results

**Climate variables assessed - 3 categories**

```{r climatevars1}
 df%>%
  select(Reference_ID, Environmental_condition)%>%
  distinct() %>% 
  count(Environmental_condition) %>%  
  group_by(Environmental_condition) %>% 
  mutate(prop = n/length(unique(df$Reference_ID)))
```

**Climate variables assessed**

```{r climatevars2}

df%>%
  select(Reference_ID, Environmental_condition)%>%
  distinct()%>%
  mutate(measured = Environmental_condition)%>%
  mutate(measured = ifelse(measured == 'Temperature', 'T', ifelse(measured == 'Precipitation', 'P', 'H')))%>%
  pivot_wider(names_from=Environmental_condition, values_from=measured ,values_fill='')%>%
  relocate(any_of(c('Reference_ID', 'Temperature', 'Precipitation', 'Humidity')))%>%
  mutate(ClimMeasured = paste0(Temperature, Precipitation, Humidity))%>%
  group_by(ClimMeasured)%>%
    summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 
  
```

**Proportion of each disease group in dataset**

```{r studycounts1}

df%>%
  filter(General_Disease != "Multiple") %>%
  group_by(General_Disease) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N))

```

**Proportion of specific diseases in dataset**

```{r studycounts2}

tmp = df%>%
  filter(General_Disease != "Multiple") %>%
  group_by(Disease, General_Disease) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N))
print(tmp,n = length(tmp$N))

```

**Response variables examined**

```{r responsecounts}

df%>%
  group_by(General_response) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Host Taxonomic Groups**

```{r taxgroups}
df%>%
  group_by(Principal_reservoir) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Type of Pathogen**

```{r pathtype}

df%>%
  group_by(Pathogen) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 
```

**Linear vs Non-linear**

```{r lnl}
df%>%
  group_by(Linear_Nonlinear) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

```

**Statistical Method**

```{r statsmeth2}
tmp = df%>%
  group_by(General_Stats_Method) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

print(tmp, n = length(tmp$N))
```

**Statistical Method**

```{r statsmeth}
tmp = df%>%
  group_by(Statistical_method) %>%
  summarise(N = n_distinct(Reference_ID)) %>%#
  mutate(P = N/sum(N))%>%
  arrange(desc(N)) 

print(tmp, n = length(tmp$N))

```

# Results Section 2: General findings

### Repoted significance

Calculates reported significance of studies at different thresholds of p-value

```{r repsignificance}

#filter out records without P values
tmp = df %>% filter(P.value_general!="" & !is.na(P.value_general))
tmp %<>% filter(!is.na(Value))
#unify P-values
tmp$P.value_general[tmp$P.value_general=="<.0.05" |
                    tmp$P.value_general=="<0..05"] = "<0.05"
tmp = as.data.frame(table(tmp$P.value_general)); names(tmp) = c("significance","freq")

#calculate proportions
tmp %<>% mutate(P = freq/sum(freq))

#Proportion of studies significant at different levels
cat("Proportion of studies significant at alfa = 0.05: ", tmp %>% filter(significance!=">0.05") %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum), "\n")

cat("Proportion of studies significant at alfa = 0.01: ", tmp %>% filter(!significance %in% c("<0.05",">0.05")) %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum),"\n")

cat("Proportion of studies significant at alfa = 0.001: ", tmp %>% filter(!significance %in% c("<0.05",">0.05","<0.01")) %>% summarise(total_sum = sum(P, na.rm = TRUE)) %>% pull(total_sum),"\n")

```

### Chi-square direction reported + groups removed

Calculates chi-square tests looking into differences in proprortions of reported increased and decreases in zoonotic risk across different groups.

```{r chi_square}
source(here::here('scripts','analysis', '01_Direction_ChiSquare_Tests.R'))

print(chi_all)


cat("Ratio of increase to decrease:", print(unname((table(df$Direction)["increase"]/table(df$Direction)["decrease"][[1]]))))

print(unname((table(df$Direction[df$Transmission_type=="Vectored"])["increase"]/table(df$Direction[df$Transmission_type=="Vectored"])["decrease"][[1]])))
table(df$Direction[df$Transmission_type=="Vectored"])

print(unname((table(df$Direction[df$Transmission_type=="Non-vectored"])["increase"]/table(df$Direction[df$Transmission_type=="Non-vectored"])["decrease"][[1]])))
table(df$Direction[df$Transmission_type=="Non-vectored"])

```

### Secondary reporting rates

Looks into proportions of studies that report more than 1 statistic and how many of these are statistically significant.

```{r secondary_reporting_rates}
# Filter out records without P values
second <- df %>% 
  filter(P.value_general != "" & !is.na(P.value_general))

# Unify P-values
second <- second %>%
  mutate(P.value_general = ifelse(P.value_general %in% c("<.0.05", "<0..05", 
                            "<0.0001", "<0.001","<0.01"), "<0.05", P.value_general))
# Save IDs of unique studies
studies <- unique(second$Reference_ID)

# Create a dataframe to store results
studies_with_secondary <- data.frame(studies = studies, secondary = NA, 
                                     any_significant = NA,two_or_more_significant = NA)
# Evaluate reporting rates
for(i in studies_with_secondary$studies) {
  
  sub <- second %>% filter(Reference_ID == i) # Subset records for one study
  levels <- sub$P.value_general # List significance levels within the study
  
  # Note studies with 2 or more significance values reported
  studies_with_secondary$secondary[studies_with_secondary$studies == i] <- length(levels) > 1
  
  # Note studies with at least 1 significant result
  studies_with_secondary$any_significant[studies_with_secondary$studies == i] <- "<0.05" %in% levels
  
  # Note studies with more than 1 significant result
  studies_with_secondary$two_or_more_significant[studies_with_secondary$studies == i] <- sum(levels == "<0.05") > 1
}

cat(sum(studies_with_secondary$secondary), "studies with secondary reporting")

cat(sum(studies_with_secondary$any_significant[studies_with_secondary$secondary==TRUE]), "studies with secondary reporting had at least 1 stat, with p < 0.05")

cat(sum(studies_with_secondary$two_or_more_significant[studies_with_secondary$secondary==TRUE]), "had 2 or more stats. significant at <0.05")
```

# Results Section 3: Effect Size analysis

##Effect Size Category Grouping

```{r numpergroup}

# group effect sizes
df = df%>%
  mutate(es_cat = ifelse(es < -0.2, 'Negative effect (g < -0.2)', #if CI contains 0
                         ifelse(abs(es) < 0.2, 'No effect', # otherwise negative
                                'Positive effect (g > 0.2)')) # or positive
  )

df%>%#
  filter(!is.na(es)) %>% 
  #subset(Linear_Nonlinear == 'Linear')%>%
  count(es_cat, Environmental_condition)%>%
  group_by(Environmental_condition)%>%
  mutate(prop = n /sum(n), tot = sum(n))
```

## Determining how category distributions differ from full dataset

Sourced scripts creates two objects: `results_df` for AD Tests and `ks_results` for Two-sample Kolmogorov-Smirnov test.

```{r}
source(here('scripts', '01_AD_KS_Multiple_Testing_Correction.R'))

```

## Original vs transformed stats - checking agreement

```{r}

new_stats = df%>% filter(Direction!="" & es_cat!="" & 
                     !is.na(Direction) & !is.na(es_cat)) #filter(P.value_general!=">0.05")

# Set the number of bootstrap samples
num_bootstrap_samples <- 1000

# Initialize vectors to store p-values
p_values <- numeric(num_bootstrap_samples)

# Perform bootstrapping with 80% of the data
set.seed(123)  # For reproducibility
for (i in 1:num_bootstrap_samples) {
  # Resample 80% of the data with replacement
  resampled_data <- new_stats %>% sample_frac(size = 0.8, replace = TRUE)
  
  # Create a contingency table
  contingency_table <- table(resampled_data$Direction, resampled_data$es_cat)
  
  # Perform Fisher's exact test with increased workspace
  fisher_test <- fisher.test(contingency_table, workspace = 2e8)
  
  # Store the p-value
  p_values[i] <- fisher_test$p.value
}

# Summarize the results
p_value_summary <- summary(p_values)
p_value_summary
mean(p_values)
```

# Results section 4: Spatial Climate Data

First need to run the `02_climate_variables.R` script and get the `es_climvars_proportions.csv` dataframe. The script below run the Chi-square & Fisher's Exact tests for Count Data on the contingency tables describing The dominant climate change ranges for sites where disease climate sensitivity was reported.

Produces `dat_results` table summarising test results.

```{r}
source(here::here('scripts','03_Hedge_vs_ClimChange_Tests.R'))
print(dat_results)

# Save the summary table.
dat_results %<>% filter(list_name!="temp_05")
# write.csv(dat_results, here('outputs', 'tables', 'TableS8_climvars_hedge_tests.csv'), row.names = F)
```

# Results section 5: Publication bias & limitations

### Eggers test

Standardise Journal names and fetch open metrics (OpenAlex)

```{r}
library(dplyr); library(stringr); library(stringi)

# 1) Robust normaliser for matching keys
norm_key <- function(x){
  x %>%
    # strip common mojibake/zero-widths/BOM/NBSP and the notorious 'Â'
    str_replace_all("[\\u00A0\\u00AD\\u200B-\\u200D\\u2060\\uFEFF\\u00C2]", "") %>%
    # normalise punctuation like en-dashes/curly quotes
    stri_trans_general("Any-ASCII") %>%
    str_to_lower() %>%
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

# 2) Canonical map (keys are normalised with norm_key)
fix_map <- c(
  # canonical case normalisations
  "emerging infectious diseases" = "Emerging Infectious Diseases",
  "nature communications" = "Nature Communications",
  "plos one" = "PLOS ONE",
  "plos neglected tropical diseases" = "PLOS Neglected Tropical Diseases",
  "frontiers in veterinary science" = "Frontiers in Veterinary Science",

  # accents / language
  "cadernos de saude publica" = "Cadernos de Saúde Pública",
  "ciencia & saude coletiva" = "Ciência & Saúde Coletiva",
  "epidemiologia e servicos de saude" = "Epidemiologia e Serviços de Saúde",

  # publisher/series tidy-ups
  "mdpi viruses" = "Viruses",
  "bmc environmental health" = "Environmental Health",
  "bmc parasites & vectors" = "Parasites & Vectors",
  "environmental research and public health" = "International Journal of Environmental Research and Public Health",
  "environmental and public health" = "Journal of Environmental and Public Health",
  "proceedings of the national academy of sciences, biological sciences" = "Proceedings of the National Academy of Sciences",
  "proceedings of the royal society b" = "Proceedings of the Royal Society B: Biological Sciences",
  "proceedings of the royal society b: biological sciences" = "Proceedings of the Royal Society B: Biological Sciences",
  "philosophical transaction of the royal society b: biological sciences" = "Philosophical Transactions of the Royal Society B: Biological Sciences",
  "mary ann liebert: vector-borne and zoonotic diseases" = "Vector-Borne and Zoonotic Diseases",

  # typos
  "journal of vetinary science" = "Journal of Veterinary Science",
  "vetinary journal" = "The Veterinary Journal",
  "j arthropod-borne dis" = "Journal of Arthropod-Borne Diseases",
  "journal of arthropod borne diseases" = "Journal of Arthropod-Borne Diseases",

  # style nits
  "science of the total environment" = "Science of the Total Environment",
  "ambio: a journal of environment and society" = "Ambio",

  # *** explicit catch for your mojibake case(s) ***
  "emerging microbes & infections" = "Emerging Microbes & Infections",
  "emerging microbes & infectionsa" = "Emerging Microbes & Infections"  # when 'Â' became 'A'
)

df <- df %>%
  mutate(
    .key_raw = Journal,
    .key = norm_key(Journal),
    Journal_std = if_else(.key %in% names(fix_map),
                          unname(fix_map[.key]),
                          str_squish(Journal)),
    Journal_needs_review = .key %in% norm_key(c("DovePress"))   # keep flagging publishers
  ) %>%
  select(-.key)

View(df %>% select(Journal, Journal_std, Journal_needs_review, Journal_5yr_Impact) %>% distinct(
))

```

```{r journal_standardization, message=TRUE, warning=FALSE}
# install.packages("openalexR")
library(openalexR)
library(dplyr); library(purrr); library(stringr)

journals <- df %>% distinct(Journal_std) %>% filter(!is.na(Journal_std)) %>% pull()
ifs_scored = df %>% select(Journal_5yr_Impact, Journal_std) %>% filter(!is.na(Journal_5yr_Impact)) %>% distinct()
length(unique(ifs_scored$Journal_std))
df$Journal_5yr_Impact_merged = NA

for(i in unique(ifs_scored$Journal_std)){
  score = ifs_scored$Journal_5yr_Impact[ifs_scored$Journal_std==i]
  df$Journal_5yr_Impact_merged[df$Journal_std==i] = score
}

View(df %>% select(Journal_std, Journal_5yr_Impact_merged) %>% distinct())

mailto <- "arturtrebski9924@gmail.com"  # <-- put your email here (OpenAlex requests it)
# install.packages(c("httr2","jsonlite","dplyr","purrr","stringr","tibble"))
# install.packages(c("jsonlite","dplyr","tibble"))
library(jsonlite); library(dplyr); library(tibble)

`%||%` <- function(x, y) if (is.null(x)) y else x
collapse_issn <- function(v) if (is.null(v) || length(v)==0) NA_character_ else paste(v, collapse=";")

oa_sources_search <- function(name, per_page = 10, mailto = "arturtrebski9924@gmail.com", pause = 0.15) {
  qs  <- paste0("search=", utils::URLencode(name, reserved = TRUE),
                "&per-page=", per_page,
                "&mailto=", utils::URLencode(mailto, reserved = TRUE))
  url <- paste0("https://api.openalex.org/sources?", qs)

  js <- tryCatch(jsonlite::fromJSON(url, simplifyVector = FALSE), error = function(e) NULL)
  if (is.null(js) || is.null(js$results) || length(js$results) == 0) return(tibble())

  rows <- do.call(rbind, lapply(js$results, function(x) {
    tibble(
      Journal_std   = name,
      oa_display_name = x$display_name %||% NA_character_,
      type            = x$type %||% NA_character_,
      issn_l          = x$issn_l %||% NA_character_,
      issn            = collapse_issn(x$issn),
      publisher       = x$host_organization_name %||% NA_character_,
      works_count     = x$works_count %||% NA_real_,
      openalex_id     = x$id %||% NA_character_,
      oa_2yr_mean_citedness = (x$summary_stats %||% list())[["2yr_mean_citedness"]] %||% NA_real_,
      oa_h_index            = as.integer((x$summary_stats %||% list())[["h_index"]] %||% NA_real_),
      oa_i10_index          = as.integer((x$summary_stats %||% list())[["i10_index"]] %||% NA_real_)
    )
  }))

  out <- rows |>
    mutate(
      exact = tolower(oa_display_name) == tolower(name),
      dist  = adist(tolower(oa_display_name), tolower(name))[,1],
      is_journal = type == "journal"
    ) |>
    arrange(desc(exact), desc(is_journal), dist, desc(works_count)) |>
    slice(1) |>
    select(Journal_std, oa_display_name, issn_l, issn, publisher, openalex_id,
           oa_2yr_mean_citedness, oa_h_index, oa_i10_index)

  Sys.sleep(pause)
  out
}

# Sanity checks (each should return 1 tidy row)
oa_sources_search("Nature Communications")
oa_sources_search("Emerging Infectious Diseases")
oa_sources_search("Emerging Microbes & Infections")

# Map across your cleaned journals and join back
journals <- df |>
  filter(!is.na(Journal_std), Journal_std != "") |>
  distinct(Journal_std) |>
  pull()

safe_oa <- function(nm) tryCatch(oa_sources_search(nm), error = function(e) tibble())
oa_tbl  <- do.call(bind_rows, lapply(journals, safe_oa))

df_with_metrics <- df |> left_join(oa_tbl, by = "Journal_std")
View(df_with_metrics %>% select(Journal_std, oa_2yr_mean_citedness, oa_h_index, Journal_5yr_Impact_merged) %>% distinct())

```

```{r eggers_tests}

f1 <- df %>% filter(!is.na(se) & !is.na(es))
f2 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Temperature")
f3 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Precipitation")
f4 <- df %>% filter(!is.na(se) & !is.na(es)) %>% filter(Environmental_condition == "Humidity")

regtest_all = regtest(f1$es, sei=f1$se)
regtest_temp = regtest(f2$es, sei=f2$se)
regtest_prec = regtest(f3$es, sei=f3$se)
regtest_hum = regtest(f4$es, sei=f4$se)

list_regtest = list(All = regtest_all$pval, Temperature = regtest_temp$pval, Precipitation = regtest_prec$pval, Humidity = regtest_hum$pval)

print("P-values of regtests:")
print(list_regtest)

#--------------------------------------
# Extract the p-values and z-values
pval_all <- regtest_all$pval
zval_all <- regtest_all$zval
est_all <- regtest_all$est

pval_temp <- regtest_temp$pval
zval_temp <- regtest_temp$zval
est_temp <- regtest_temp$est

pval_prec <- regtest_prec$pval
zval_prec <- regtest_prec$zval
est_prec <- regtest_prec$est

pval_hum <- regtest_hum$pval
zval_hum <- regtest_hum$zval
est_hum <- regtest_hum$est

# Create a data frame
results_regtests <- data.frame(
  Data = c("Overall_dataset", "Temperature", "Precipitation", "Humidity"),
  Intercept = c(est_all, est_temp, est_prec, est_hum),
  Z_value = c(zval_all, zval_temp, zval_prec, zval_hum),
  P_value = c(pval_all, pval_temp, pval_prec, pval_hum)
)

write.csv(results_regtests, here('outputs', 'tables_new', 'TableS9_eggers_funnel_tests.csv'), row.names = F)
```

## IF vs Effect Size - looking for publication bias

```{r journals, echo=TRUE, message=TRUE, warning=TRUE}

library(ggplot2)
library(dplyr)

# ES VS IF ---

## Prepare data
pub1 = df %>% filter(!is.na(es) & !is.na(Journal_5yr_Impact))
pub1 %<>% dplyr::select(es, Journal_5yr_Impact, Environmental_condition)
dat_pub1 = pub1
dat_pub1$es = abs(dat_pub1$es)

## Perform Shapiro-Wilk test for normality ----
print(shapiro.test(abs(dat_pub1$es)))
print(shapiro.test((dat_pub1$Journal_5yr_Impact)))

# Do correlation test for all data points
spearman_corr <- cor.test(dat_pub1$Journal_5yr_Impact, 
                          abs(dat_pub1$es), 
                          method = "spearman", 
                          exact = FALSE)
print(spearman_corr)

## Check outliers-------------
# IF 
# Calculate IQR
Q1 <- quantile(dat_pub1$Journal_5yr_Impact, 0.25)
Q3 <- quantile(dat_pub1$Journal_5yr_Impact, 0.75)
IQR <- Q3 - Q1

# Check outliers using IQR
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
iqr_outliers <- dat_pub1 %>% filter(Journal_5yr_Impact < lower_bound |
                                     Journal_5yr_Impact > upper_bound)
# Print the number of outliers among Impact Factors
cat("Number of outliers detected by IQR method:", nrow(iqr_outliers), "\n")

# Print the outliers detected
cat("Outliers detected by IQR method:\n")
print(unique(iqr_outliers$Journal_5yr_Impact))

# Remove outliers
dat_pub1 %<>% filter(!Journal_5yr_Impact %in% unique(iqr_outliers1$Journal_5yr_Impact))

# ES ! ----
# Calculate IQR
Q1 <- quantile(dat_pub1$es, 0.25)
Q3 <- quantile(dat_pub1$es, 0.75)
IQR <- Q3 - Q1

# Check outliers using IQR
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
iqr_outliers1 <- dat_pub1 %>% filter(es < lower_bound | es > upper_bound)

# Print the number of outliers
cat("Number of outliers detected by IQR method:", nrow(iqr_outliers1), "\n")

# Print the outliers detected
cat("Outliers detected by IQR method:\n")
print(unique(iqr_outliers1$es))

dat_pub1 %<>% filter(!es %in% unique(iqr_outliers1$es))

# Calculate Spearman's rank correlation --

spearman_corr <- cor.test(dat_pub1$Journal_5yr_Impact, 
                          abs(dat_pub1$es), 
                          method = "spearman", 
                          exact = FALSE)
print(spearman_corr)
```

## IF vs p-value

```{r}
# p-VALUE VS IF ------------------------------------------------
## Prepare data
pub2 = df %>% filter(!is.na(P.value_specific) & !is.na(Journal_5yr_Impact))
pub2 %<>% dplyr::select(P.value_specific, Journal_5yr_Impact, Environmental_condition)
dat_pub2 = pub2
dat_pub2$P.value_specific = abs(dat_pub2$P.value_specific)

# Perform Shapiro-Wilk test for normality
shapiro_test_es <- shapiro.test((dat_pub2$P.value_specific))
shapiro_test_impact <- shapiro.test(dat_pub2$Journal_5yr_Impact)

# Print the Shapiro-Wilk test results
print(shapiro_test_es)
print(shapiro_test_impact)

# Calculate Spearman's rank correlation with outliers
spearman_corr <- cor.test(dat_pub2$P.value_specific, dat_pub2$Journal_5yr_Impact, method = "spearman", exact = FALSE)
print(spearman_corr)

## Check outliers ---------------------------------------------
# IF ! ----|
# Calculate IQR
Q1 <- quantile(dat_pub2$Journal_5yr_Impact, 0.25)
Q3 <- quantile(dat_pub2$Journal_5yr_Impact, 0.75)
IQR <- Q3 - Q1

# Check outliers using IQR
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
iqr_outliers2 <- dat_pub2 %>% filter(Journal_5yr_Impact < lower_bound | Journal_5yr_Impact > upper_bound)

# Print the number of outliers
cat("Number of outliers detected by IQR method:", nrow(iqr_outliers2), "\n")

# Print the outliers detected
cat("Outliers detected by IQR method:\n")
print(unique(iqr_outliers2$Journal_5yr_Impact))

dat_pub2 %<>% filter(!Journal_5yr_Impact %in% unique(iqr_outliers2$Journal_5yr_Impact))

# P.value_specific ! ----|
# Calculate IQR
Q1 <- quantile(dat_pub2$P.value_specific, 0.25)
Q3 <- quantile(dat_pub2$P.value_specific, 0.75)
IQR <- Q3 - Q1

# Check outliers using IQR
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
iqr_outliers2 <- dat_pub2 %>% filter(P.value_specific < lower_bound | P.value_specific > upper_bound)

# Print the number of outliers
cat("Number of outliers detected by IQR method:", nrow(iqr_outliers2), "\n")

# Print the outliers detected
cat("Outliers detected by IQR method:\n")
print(unique(iqr_outliers2$P.value_specific))

dat_pub2 %<>% filter(!P.value_specific %in% unique(iqr_outliers2$P.value_specific))

# Calculate Spearman's rank correlation without outliers
spearman_corr <- cor.test(dat_pub2$P.value_specific, dat_pub2$Journal_5yr_Impact, method = "spearman", exact = FALSE)
print(spearman_corr)
```

# Main figures

## Figure 1

```{r fig1, echo=TRUE, fig.height=9.5, fig.width=10.5}
source(here::here('scripts','figures','Figure1.R'))

plot(Fig1_new)
plot(Fig1_smalltext)

```

## Figure 2

```{r fig2, include=FALSE}
source(here::here('scripts','analysis', '01_AD_KS_Multiple_Testing_Correction.R'))
source(here::here('scripts','figures','Fig_2_fin.R'))


print(f2_small)

ggsave(f2_small, file=here::here('outputs','final_figs_new', 'Figure2.pdf'), 
        device="pdf", units="cm", width=11, height=10, dpi=600, scale=0.88)
# ggsave(f2, file=here::here('outputs', 'Figure2.png'), 
#        device="png", units="in", width=11, height=10, dpi=600, scale=0.88)
```

## Figure 3

```{r fig3, echo=FALSE}
source(here::here('scripts','Figure3.R'))

print(f3)

ggsave(f3, file=here('outputs','final_figs_new','Figure3_2.png'), device="png", units="in", width=8, height=6, dpi=600, scale=1.1)
ggsave(f3, file=here('outputs','final_figs_new','Figure3_2.pdf'), device="pdf", units="in", width=8, height=6, dpi=600, scale=1.1)
```

## Figure 4

```{r fig4, echo=FALSE}
source(here::here('scripts','Figure4.R'))

print(f4)

#ggsave(f4, file=here('outputs','Figure4.png'), device="png", units="in", width=8, height=6, dpi=300, scale=1.25)
ggsave(f4, file=here('outputs','Figure4.pdf'), device="pdf", units="in", width=8, height=6, dpi=600, scale=1.25)
```

# Supplementary figures

## Figure S2: Impact Factors

```{r FigS2}
source(here::here('scripts','FigureS2.R'))


FS2 <- (p3 / plot_spacer() /p3_rm) + plot_layout(heights = c(1,0.05,1))

ggsave(FS2, file=here('outputs','FigureS2.pdf'), device="pdf", 
       units="in", width=9, height=8, dpi=1000, scale=1.1)

#ggsave(p3, file=here('outputs','FigureS2_AB.png'), device="png", units="in", width=9.3, height=4, dpi=1000, scale=1)
#ggsave(p3_rm, file=here('outputs','FigureS2_CD.png'), device="png", units="in", width=9.3, height=4, dpi=1000, scale=1)
```

## Figure S3: Funnel plots

```{r FigS3}

source(here::here('scripts','FigureS3.R'))

print(s3)

#ggsave(s3, file=here('outputs','FigureS3.png'), device="png", units="in", width=8.5, height=5, dpi=1000, scale=1)
ggsave(s3, file=here('outputs','FigureS3.pdf'), device="pdf", units="in", width=7.5, height=5, dpi=1000, scale=1.2)
```

## Figure S4: p-val distribution

```{r FigS4}
source(here::here('scripts','FigureS4.R'))

print(combined_plot_s4)

#ggsave(combined_plot_s4, file="outputs/FigureS4.png", device="png", units="in", width=6.3, height=3.8, dpi=1000, scale=0.95)
ggsave(combined_plot_s4, file=here('outputs','FigureS4.pdf'), device="pdf", units="in", width=6.3, height=3.8, dpi=1000, scale=1.1)
```

## Figure S5: Original vs transformed stats

```{r FigS4}
source(here::here('scripts','FigureS5.R'))

print(plot_s5)

#ggsave(here::here('outputs','FigureS5_new.png'), plot_s5, width = 5.5, height = 6.7, units = "in", dpi = 300, scale = 1.7)

ggsave(here::here('outputs','FigureS5.pdf'), plot_s5, device = "pdf",
       width = 5.5, height = 6.7, units = "in", dpi = 600, scale = 1.7)
```

## Figure S6: Hedge's g categories vs climate change ranges

```{r Fig S6}
# Produces combined_plot_S6
source(here::here('scripts','FigureS6.R'))

print(combined_plot_S6)

# Save the plot
ggsave(combined_plot_S6, file=here('outputs', 'FigureS6.pdf'), device="pdf", units="in", width=11, height=8, dpi=1000, scale=1) 
```
