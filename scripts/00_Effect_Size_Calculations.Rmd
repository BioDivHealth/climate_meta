---
title: "Effect Size Transformation"
author: "Lewis Gourlay"
date: "2023-04-03"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Set-up of packages


```{r Loading libraries, include=FALSE}
library(pacman)
p_load(esc, dplyr, gratful, ggplot2, cowplot, 
       testthat, patchwork, tidyr, metafor, maps, here)

```

# Data preparation

```{r setting directory, include=FALSE}

# Read in the data
df <- read.csv(here::here('data','dataset_final_new.csv'), header = TRUE, stringsAsFactors = FALSE, fileEncoding = "latin1")
zooALL <- read.csv(here::here('data','dataset_final_new.csv'), header = TRUE, stringsAsFactors = FALSE, fileEncoding = "latin1")

zooALL$Value[zooALL$Value==""]<-NA
zooALL$Value[zooALL$Value=="NA"]<-NA
zooALL$Upper[zooALL$Upper==""]<-NA
zooALL$Upper[zooALL$Upper=="NA"]<-NA
zooALL$Lower[zooALL$Lower==""]<-NA
zooALL$Lower[zooALL$Lower=="NA"]<-NA


# Select relevant columns and rename 'Data_ID' to 'study' for merging datasets
zooALL <- zooALL %>%
  select(-es, -se, -weight, -sample.size, -var, -ci.lo, -ci.hi) %>%
  dplyr::rename(study = Data_ID)

# Keep an unedited version of the dataset, keep only unique rows
zooALL_copy <- unique(zooALL)
zooALL = unique(zooALL)

zooALL <- zooALL %>%
  filter(!is.na(Value)) %>%
  filter(Type == "Percent change" | (Value > -25 & Value < 25))
```

```{r Convert environmental variables to the same scale, include=FALSE}

zooALL2<-zooALL #create copy dataset
zooALL2$Specific_scale[zooALL2$Specific_scale==""]<-1

# Convert 'Specific_scale' to factor and remove non-numerical values
zooALL2$Specific_scale <- as.factor(zooALL2$Specific_scale)
zooALL2 <- subset(zooALL2, !Specific_scale %in% c("ln(mm)", "Log(degrees)", "Log(mm)", "mm^3/yr"))

# Summary of 'Specific_scale'
summary(zooALL2$Specific_scale)

# Create a new data frame for calculations
zooALL3 <- data.frame(Factorial = zooALL2$Specific_scale, Value = zooALL2$Value, Lower = zooALL2$Lower, Upper = zooALL2$Upper)

# Convert 'Upper' and 'Lower' to numeric
zooALL3$Upper <- as.numeric(zooALL3$Upper)
zooALL3$Lower <- as.numeric(zooALL3$Lower)

# Function to apply conversion
convert_calculation_value <- function(factorial, value) {
  if (!is.na(factorial)) {
    return(value / as.numeric(as.character(factorial)))
  } else {
    return(value)
  }
}

# Apply conversions to 'Value', 'Upper', and 'Lower'
zooALL3$value_new <- mapply(convert_calculation_value, zooALL3$Factorial, zooALL3$Value)
zooALL3$Upper_new <- mapply(convert_calculation_value, zooALL3$Factorial, zooALL3$Upper)
zooALL3$Lower_new <- mapply(convert_calculation_value, zooALL3$Factorial, zooALL3$Lower)

# Function to unlist and handle NULL values
unlist2 <- function(a.list) {
  return(do.call(c, lapply(a.list, function(x) {
    if (is.null(x) | length(x) == 0) {
      NA
    } else {
      x
    }
  })))
}

# Update 'Value', 'Upper', and 'Lower' in zooALL2  
zooALL2$Value<-unlist2(zooALL3$value_new)
zooALL2$Upper<-unlist2(zooALL3$Upper_new)
zooALL2$Lower<-unlist2(zooALL3$Lower_new)

# Use the dataset with numerical scales
zooALL <- zooALL2
```

```{r composition of final dataset}

# Count the number of independent studies
num_studies <- zooALL %>%
  filter(!is.na(Reference_ID)) %>%
  summarise(num_studies = n_distinct(Reference_ID))

# Print the result
cat("Number of independent studies:",num_studies$num_studies, "\n")

zooALL_raw = zooALL

```

```{r Function to convert standard error to standard deviation, include = TRUE}

# Define the function to convert standard error to standard deviation
se_to_sd <- function(se, n) {
  se * sqrt(n)
}

```

```{r Function to convert 95% Confidence interval to Standard deviation, include = TRUE}

# Define the function to convert confidence interval bounds to standard deviation
ci_to_sd <- function(ci_lower, ci_upper, n, conf_level = 0.95) {
  # Calculate the critical value from the normal distribution
  crit_val <- qnorm((1 + conf_level) / 2)
  # Convert confidence interval to standard deviation
  sd <- sqrt(n) * (ci_upper - ci_lower) / (2 * crit_val)
  return(sd)
}

```

# Helpers 

```{r}
# --- CI divisor helper (supports 90/95/99% CI & 'Credible' wording)
ci_divisor <- function(err_label) {
  if (is.na(err_label)) return(NA_real_)
  lab <- tolower(err_label)
  if (grepl("90%.*(ci|credible)", lab)) return(2*1.645)   # 3.29
  if (grepl("95%.*(ci|confidence|credible)", lab)) return(2*1.96)   # 3.92
  if (grepl("99%.*(ci|credible)", lab)) return(2*2.576)  # 5.152
  NA_real_
}


se_beta_from_error <- function(Error, Std_error, Upper, Lower) {
  # flags
  is_se  <- !is.na(Error) && grepl("^\\s*std\\.?\\s*error\\s*$", Error, ignore.case = TRUE)
  is_sd  <- !is.na(Error) && grepl("^\\s*std\\.?\\s*dev\\s*$",   Error, ignore.case = TRUE)
  is_var <- !is.na(Error) && grepl("variance",                   Error, ignore.case = TRUE)

  # 1) explicit Std. error
  if (is_se && !is.na(Std_error)) return(abs(Std_error))

  # 2) labelled CI (90/95/99)
  div <- ci_divisor(Error)
  if (!is.na(div) && !is.na(Upper) && !is.na(Lower)) return(abs(Upper - Lower) / div)

  # 2b) "Std. error" label but SE missing, bounds present -> assume 95% CI
  if (is_se && is.na(Std_error) && !is.na(Upper) && !is.na(Lower))
    return(abs(Upper - Lower) / 3.92)

  # 2c)generic unlabeled-bounds fallback (assume 95% CI) when no SE and no CI label
  if (is.na(div) && is.na(Std_error) && !is.na(Upper) && !is.na(Lower))
    return(abs(Upper - Lower) / 3.92)

  # 3) Std. dev (posterior SD) as SE, or ±1 SD bounds
  if (is_sd && !is.na(Std_error)) return(abs(Std_error))
  if (is_sd && is.na(Std_error) && !is.na(Upper) && !is.na(Lower)) return(abs(Upper - Lower) / 2)

  # 4) (Unconditional) variance
  if (is_var && !is.na(Std_error)) return(sqrt(abs(Std_error)))

  # 5) Missing label but numeric Std._error
  if (is.na(Error) && !is.na(Std_error)) return(abs(Std_error))

  NA_real_
}


```

# Effect sizes


### Odds ratio new

```{r}
library(dplyr)
library(stringr)
library(dplyr)
library(stringr)

p_floor <- 1e-16

OR <- zooALL %>%
  filter(Type == "Odds ratio") %>%
  mutate(
    Value      = as.numeric(Value),
    Lower      = as.numeric(Lower),
    Upper      = as.numeric(Upper),
    Std._error = as.numeric(Std._error),
    N.value    = as.numeric(N.value),

    # --- Clean p-values ---
    p_spec_raw = tolower(trimws(as.character(P.value_specific))),
    p_gen_raw  = tolower(trimws(as.character(P.value_general))),

    # numeric forms if directly parseable
    p_spec_num = suppressWarnings(as.numeric(p_spec_raw)),
    p_gen_num  = suppressWarnings(as.numeric(p_gen_raw)),

    # bounds from strings like "<0.01" or ">0.05"
    p_gen_lt = ifelse(str_detect(p_gen_raw, "^<\\s*"),
                      suppressWarnings(as.numeric(str_extract(p_gen_raw, "[0-9]+\\.?[0-9eE-]*"))),
                      NA_real_),
    p_gen_gt = ifelse(str_detect(p_gen_raw, "^>\\s*"),
                      suppressWarnings(as.numeric(str_extract(p_gen_raw, "[0-9]+\\.?[0-9eE-]*"))),
                      NA_real_),

    # sanitize numerics
    p_spec_num = case_when(
      is.na(p_spec_num) ~ NA_real_,
      p_spec_num <= 0 ~ p_floor,              # avoid zero
      p_spec_num >= 1 ~ NA_real_,
      TRUE ~ p_spec_num
    ),
    p_gen_num = case_when(
      is.na(p_gen_num) ~ NA_real_,
      p_gen_num <= 0 ~ p_floor,
      p_gen_num >= 1 ~ NA_real_,
      TRUE ~ p_gen_num
    ),
    p_gen_lt = case_when(
      is.na(p_gen_lt) ~ NA_real_,
      p_gen_lt <= 0 ~ p_floor,
      p_gen_lt >= 1 ~ NA_real_,
      TRUE ~ p_gen_lt
    ),

    # choose a p to use (only as fallback for SE)
    p_use = coalesce(
      p_spec_num,            # 1) specific numeric
      p_gen_num,             # 2) general numeric
      p_gen_lt               # 3) general "<x" (conservative upper bound)
      # NOTE: we intentionally DO NOT use ">x"
    ),

    p_source = case_when(
      !is.na(p_spec_num) ~ "p_specific",
      is.na(p_spec_num) & !is.na(p_gen_num) ~ "p_general_numeric",
      is.na(p_spec_num) & is.na(p_gen_num) & !is.na(p_gen_lt) ~ "p_general_lt",
      TRUE ~ "none"
    ),

    err_norm   = str_to_lower(coalesce(Error, "")),
    valid_ci   = !is.na(Lower) & !is.na(Upper) & Lower > 0 & Upper > 0 & Upper > Lower,

    # --- Pick SE for log(OR) in priority order ---
    se_log_or = case_when(
      valid_ci & str_detect(err_norm, "95") ~ (log(Upper) - log(Lower)) / (2 * qnorm(0.975)),
      !is.na(Std._error) ~ Std._error,  # assume this is SE of log(OR) when present
      !is.na(p_use) & Value > 0 ~ abs(log(Value)) / qnorm(1 - p_use/2),
      TRUE ~ NA_real_
    ),

    # --- OR → d (Chinn) and to Hedges g ---
    d    = ifelse(Value > 0, log(Value) * sqrt(3)/pi, NA_real_),
    se_d = ifelse(!is.na(se_log_or), (sqrt(3)/pi) * se_log_or, NA_real_),

    # small-sample correction (set J=1 if N unknown/clustered)
    J = ifelse(!is.na(N.value) & N.value > 3, 1 - 3/(4*N.value - 9), 1),
    g    = J * d,
    se_g = J * se_d,

    se_source = case_when(
      valid_ci & str_detect(err_norm, "95") ~ "from_95CI_log",
      !is.na(Std._error) ~ "from_SE_log",
      !is.na(p_use) ~ paste0("from_", p_source),
      TRUE ~ "none"
    )
  ) %>%
  # QC
  filter(!is.na(g), !is.na(se_g), is.finite(g), is.finite(se_g)) %>%
  # drop absurd or inverted CI rows if any slipped through
  filter(!(valid_ci & (Lower <= 0 | Upper <= 0 | Upper <= Lower)))

# columns for meta
OR$es <- OR$g
OR$se <- OR$se_g
OR$sample.size <- OR$N.value
OR_comp <- OR


```

#### Scatterplot odds ratio

```{r}
# -----PLOT ORIGINAL STAT VS HEDGES G --------------
OR_comp_scatter <- ggplot(data = OR_comp, aes(x = Value, y = es)) +
  geom_line() +
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) + 
  xlab("Odds Ratio") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
OR_comp$es <- unlist(OR_comp$es)
model <- lm(es ~ Value, data = OR_comp)
residuals <- model$residuals
outliers <- OR_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
OR_comp_scatter <- OR_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Red dot for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add labels only for outliers
  scale_color_manual(name = "", values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +
  scale_shape_manual(name = "", values = c("Observation" = 16, "Outlier" = 16)) +
  
  # Increase font sizes for legend and axis labels
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend font size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(OR_comp_scatter)
```

### Beta coefficient standardised new

```{r}
## --- STANDARDISED BETAS (Type == "Beta coefficient") -> HEDGES' g ---
## Uses: Value (beta*), N.value, Std._error, Upper/Lower + Error, p-values, R.squared,
##       Statistical_method, General_Stats_Method.

library(dplyr)
library(stringr)
library(tibble)

# Vectorized p-value parser (safe in mutate)
if (!exists("parse_p_vec")) {
  parse_p_vec <- function(p) {
    x <- tolower(trimws(as.character(p)))
    x <- gsub(",", ".", x); x <- gsub("\\s+", "", x)
    x[x %in% c("", "na", "n/a", "ns", "n.s.", "notsignificant", "not significant")] <- NA_character_
    x <- sub("^p\\s*[=<>≤≥]\\s*", "", x)
    x[grepl("^[>≥]", x)] <- NA_character_
    x <- sub("^[<≤=]", "", x)
    num <- suppressWarnings(as.numeric(x))
    ifelse(is.na(num), NA_real_, pmin(pmax(num, 1e-16), 1 - 1e-16))
  }
}

# r -> Hedges g (+ SE) helper (small-sample correction J)
if (!exists("hedges_from_r")) {
  hedges_from_r <- function(r, N) {
    r <- ifelse(!is.na(r), pmax(pmin(r, 1 - 1e-8), -1 + 1e-8), NA_real_)
    J <- ifelse(!is.na(N), 1 - 3/(4*N - 9), NA_real_)
    d <- ifelse(!is.na(r), 2*r / sqrt(1 - r^2), NA_real_)
    g <- ifelse(!is.na(d) & !is.na(J), J * d, NA_real_)
    se_g <- ifelse(!is.na(r) & !is.na(N) & N > 3,
                   (1 - 3/(4*N - 9)) * 2 / sqrt((1 - r^2) * (N - 3)),
                   NA_real_)
    list(g = g, se_g = se_g)
  }
}

# Map method to family for inverting p-values correctly
method_family <- function(method_chr) {
  m <- tolower(method_chr)
  case_when(
    str_detect(m, "ordinary\\s+least\\s+squares|\\bols\\b|multivariate\\s+linear|linear\\s+fixed") ~ "lm",
    str_detect(m, "logistic|poisson|negative\\s*binomial|zero\\s*inflated|general(ized|ised)\\s+linear") ~ "glm",
    str_detect(m, "mixed") ~ "glmm",
    str_detect(m, "bayesian") ~ "bayes",
    str_detect(m, "panel|fixed[- ]?effect") ~ "lm",
    TRUE ~ "other"
  )
}

# Conservative simple-OLS detector for LAST-RESORT fallback
is_simple_linear_vec <- function(method, general) {
  m <- tolower(ifelse(is.na(method),  "", as.character(method)))
  g <- tolower(ifelse(is.na(general), "", as.character(general)))
  s <- paste(m, g)
  ols_like <- grepl("\\b(ols|ordinary\\s+least\\s+squares|linear\\s+regression)\\b", s, perl = TRUE)
  uni_hint <- grepl("\\b(simple|univariate|bivariate|single\\s*predictor)\\b", s, perl = TRUE)
  multi_like <- grepl("\\b(multivariate|multiple\\s+linear|glm|general(ized|ised)\\s+linear|mixed|panel)\\b", s, perl = TRUE)
  ols_like & uni_hint & !multi_like
}

# ---- Pipeline ----
Z <- zooALL %>% filter(Type == "Beta coefficient") %>% rowid_to_column("rowid")

Beta <- Z %>%
  mutate(
    beta_star  = suppressWarnings(as.numeric(Value)),
    Upper      = suppressWarnings(as.numeric(Upper)),
    Lower      = suppressWarnings(as.numeric(Lower)),
    Std._error = suppressWarnings(as.numeric(Std._error)),
    N.value    = suppressWarnings(as.numeric(N.value)),
    R.squared  = suppressWarnings(as.numeric(R.squared)),
    # normalise possible percent R^2
    R.squared  = ifelse(!is.na(R.squared) & R.squared > 1 & R.squared <= 100, R.squared/100, R.squared),
    p_raw = dplyr::coalesce(
      if ("P.value_specific" %in% names(.)) P.value_specific else NA,
      if ("P.value_general"  %in% names(.)) P.value_general  else NA
    ),
    p_val = parse_p_vec(p_raw),
    fam   = method_family(Statistical_method)
  ) %>%
  # --- NEW: robust bound handling + SE reconstruction even when Error label is wrong/empty ---
  # mutate(
  #   # sort bounds; allow for mis-ordered or degenerate entries
  #   hi   = ifelse(!is.na(Upper) & !is.na(Lower), pmax(Upper, Lower), NA_real_),
  #   lo   = ifelse(!is.na(Upper) & !is.na(Lower), pmin(Upper, Lower), NA_real_),
  #   width = ifelse(!is.na(hi) & !is.na(lo), hi - lo, NA_real_),
  # 
  #   # labelled CI divisors
  #   ci_div_labeled = case_when(
  #     !is.na(Error) & str_detect(Error, regex("90%.*(CI|Credible)",  ignore_case = TRUE)) ~ 3.29,   # 2*1.645
  #     !is.na(Error) & str_detect(Error, regex("95%.*(CI|Confidence|Credible)", TRUE))     ~ 3.92,   # 2*1.96
  #     !is.na(Error) & str_detect(Error, regex("99%.*(CI|Credible)", TRUE))                ~ 5.152,  # 2*2.576
  #     TRUE ~ NA_real_
  #   ),
  # 
  #   # SE from *labelled* CI (if present)
  #   se_from_ci_labeled = ifelse(!is.na(width) & width > 0 & !is.na(ci_div_labeled),
  #                               width / ci_div_labeled, NA_real_),
  # 
  #   # Fallback: SE from bounds even if label says "Std. error" or is missing
  #   # Assumes 95% CI when bounds are present but label is not a CI.
  #   se_from_bounds_default = ifelse(
  #     !is.na(width) & width > 0 &
  #     (is.na(ci_div_labeled)) & (is.na(Std._error) | Std._error <= 0),
  #     width / 3.92,  # assume 95%
  #     NA_real_
  #   ),
  # 
  #   # One SE to use
  #   SE_use = coalesce(Std._error, se_from_ci_labeled, se_from_bounds_default),
  # 
  #   # Where did SE come from? (useful for auditing rows 28–51)
  #   se_source = case_when(
  #     !is.na(Std._error) ~ "reported_se",
  #     !is.na(se_from_ci_labeled) ~ "labeled_ci",
  #     !is.na(se_from_bounds_default) ~ "assumed_95pc_from_bounds",
  #     TRUE ~ NA_character_
  #   )
  # ) %>%
  # --- SE(β*) unified from Error/Std._error/CI/Std. dev/variance ---
  mutate(
    SE_use_raw = mapply(se_beta_from_error, Error, Std._error, Upper, Lower),
    SE_use     = ifelse(!is.na(SE_use_raw), abs(SE_use_raw), SE_use_raw)
  ) %>%
  # df: prefer residual df if store it; else default ≈ N - 2
  mutate(
    df_hat = if ("df" %in% names(.)) suppressWarnings(as.numeric(df)) else
             if ("Residual.df" %in% names(.)) suppressWarnings(as.numeric(Residual.df)) else
             ifelse(!is.na(N.value) & N.value > 2, N.value - 2, NA_real_)
  ) %>%
  # Build test statistic:
  mutate(
    stat_from_se = ifelse(!is.na(beta_star) & !is.na(SE_use) & SE_use > 0, beta_star / SE_use, NA_real_),
    stat_from_p  = case_when(
      !is.na(p_val) & fam %in% c("lm", "other") & !is.na(df_hat) ~ qt(1 - p_val/2, df = df_hat),
      !is.na(p_val) & fam %in% c("glm", "glmm", "bayes")        ~ qnorm(1 - p_val/2),
      TRUE ~ NA_real_
    ),
    stat_from_p  = ifelse(!is.na(stat_from_p) & !is.na(beta_star), sign(beta_star) * stat_from_p, stat_from_p),
    stat = coalesce(stat_from_se, stat_from_p)
  ) %>%
  # Convert to r:
  mutate(
    r_from_t = ifelse(!is.na(stat) & !is.na(df_hat) & df_hat > 0,
                      stat / sqrt(stat^2 + df_hat), NA_real_),

    # LAST-RESORT simple OLS fallbacks
    simple_flag = is_simple_linear_vec(Statistical_method, General_Stats_Method),
    r_from_R2   = ifelse(simple_flag & !is.na(R.squared) & !is.na(beta_star),
                         sign(beta_star) * sqrt(pmin(pmax(R.squared, 0), 1)), NA_real_),
    r_from_beta_simple = ifelse(simple_flag & !is.na(beta_star),
                                pmax(pmin(beta_star, 1 - 1e-8), -1 + 1e-8), NA_real_),

    r_raw = coalesce(r_from_t, r_from_R2, r_from_beta_simple)
  ) %>%
  # r -> g (+ SE)
  mutate(
    g    = hedges_from_r(r_raw, N.value)$g,
    se_g = hedges_from_r(r_raw, N.value)$se_g
  ) %>%
  transmute(
    rowid,
    es = g, se = se_g,
    sample.size = N.value,
    var   = ifelse(!is.na(se), se^2, NA_real_),
    ci.lo = ifelse(!is.na(es) & !is.na(se), es - 1.96*se, NA_real_),
    ci.hi = ifelse(!is.na(es) & !is.na(se), es + 1.96*se, NA_real_),
    r_est = r_raw, beta_star, fam
  )

# Join back to full table if you want the enriched columns alongside originals:
Beta_comp_full <- Z %>% left_join(Beta, by = "rowid") 
#View(Beta_comp_full %>% select(Value, N.value, Upper, Lower, Error, Std._error, es, se))
Beta_comp = Beta_comp_full %>% select(-rowid,-r_est,-beta_star, -fam) 

```

#### Scatterplot beta

```{r}
# -----PLOT ORIGINAL STAT VS HEDGES G --------------
 {if(is.null(Beta_comp$Correlation)){Beta_comp$Correlation = Beta_comp$Value}
  Beta_comp_scatter <- ggplot(data = Beta_comp %>% filter(!is.na(es)), aes(x = Correlation, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") + 
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Standardised beta coefficient (β)") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Correlation, data = Beta_comp)
residuals <- model$residuals
outliers <- Beta_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
Beta_comp_scatter <- Beta_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Correlation, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Correlation, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(Beta_comp_scatter)
}
```

### Unstandardized coefficients

#### Unstandardized coefficients NEW METHOD

```{r}
## --- Effect sizes: Unstandardised coefficients & correlations (Hedges' g only) ---
## Uses ONLY: N.value, Type, R.squared, Value, Lower, Upper, Error, Std._error
## Correlation bucket: "Cross-correlation analysis", "Simple correlation"
## Unstandardised regression bucket: all other methods when Type == "Unstandardised coefficient"

library(dplyr)
library(tibble)

correlation_methods <- c("Cross-correlation analysis", "Simple correlation")

# Add a stable row id to the full table so we keep ALL rows/columns
Z <- zooALL %>% rowid_to_column("rowid")

# Vectorized p-value parser: handles "0.03", "p=0.03", "<0.05", "≤0.01", decimal commas, etc.
parse_p_vec <- function(p) {
  x <- tolower(trimws(as.character(p)))
  x <- gsub(",", ".", x)              # decimal commas
  x <- gsub("\\s+", "", x)            # remove spaces
  x[x %in% c("", "na", "n/a", "ns", "n.s.", "not significant", "notsignificant")] <- NA_character_
  # remove a leading "p=" / "p<" / "p≤" / "p>" / "p≥"
  x <- sub("^p\\s*[=<>≤≥]\\s*", "", x)
  # if starts with ">" or "≥", we can't reconstruct t reliably -> NA
  x[grepl("^[>≥]", x)] <- NA_character_
  # strip any leading "<", "≤", or "=" now
  x <- sub("^[<≤=]", "", x)
  num <- suppressWarnings(as.numeric(x))
  # keep within (0,1) to avoid Inf t-statistics
  ifelse(is.na(num), NA_real_, pmin(pmax(num, 1e-16), 1 - 1e-16))
}

# Helper: r -> (g, se_g) with small-sample correction J
hedges_from_r <- function(r, N) {
  r <- ifelse(!is.na(r), pmax(pmin(r, 1 - 1e-8), -1 + 1e-8), NA_real_)
  J <- ifelse(!is.na(N), 1 - 3/(4*N - 9), NA_real_)
  d <- ifelse(!is.na(r), 2*r / sqrt(1 - r^2), NA_real_)
  g <- ifelse(!is.na(d) & !is.na(J), J * d, NA_real_)
  se_g <- ifelse(!is.na(r) & !is.na(N) & N > 3,
                 (1 - 3/(4*N - 9)) * 2 / sqrt((1 - r^2) * (N - 3)),
                 NA_real_)
  list(g = g, se_g = se_g)}

# Subset to rows relevant for these two buckets (but we'll join back to Z later)
B <- Z %>%
  filter(Type %in% c("Correlation coefficient", "Unstandardised coefficient")) %>%
  # exclude Pearson/Spearman here because you handle them elsewhere
  filter(!grepl("Spearman", ifelse(is.na(Statistical_method), "", Statistical_method), ignore.case = TRUE),
         !grepl("Pearson",  ifelse(is.na(Statistical_method), "", Statistical_method), ignore.case = TRUE))

## ---------- A) CORRELATION bucket: treat Value as r ----------
B_corr <- B %>%
  filter(Type == "Correlation coefficient",
         Statistical_method %in% correlation_methods) %>%
  mutate(
    N.value = suppressWarnings(as.numeric(N.value)),
    r_raw   = suppressWarnings(as.numeric(Value))
  ) %>%
  mutate(
    g     = hedges_from_r(r_raw, N.value)$g,
    se_g  = hedges_from_r(r_raw, N.value)$se_g
  ) %>%
  transmute(rowid, g, se_g, r_raw)

## ---------- B) UNSTANDARDISED REGRESSION (no SE? use CI / p / R²) ----------
B_reg <- B %>%
  filter(Type %in% c("Unstandardised coefficient", "Correlation coefficient"),
         !Statistical_method %in% correlation_methods)  %>%
  mutate(
    beta        = suppressWarnings(as.numeric(Value)),
    Upper       = suppressWarnings(as.numeric(Upper)),
    Lower       = suppressWarnings(as.numeric(Lower)),
    Std._error  = suppressWarnings(as.numeric(Std._error)),
    N.value     = suppressWarnings(as.numeric(N.value)),
    R.squared   = suppressWarnings(as.numeric(R.squared)),
    # p-values: prefer specific, else general (use your parse_p_vec)
    p_spec_raw  = if ("P.value_specific" %in% names(.)) P.value_specific else NA,
    p_gen_raw   = if ("P.value_general"  %in% names(.)) P.value_general  else NA,
    p_spec      = parse_p_vec(p_spec_raw),
    p_gen       = parse_p_vec(p_gen_raw),
    p_use       = dplyr::coalesce(p_spec, p_gen),

    # *** unified SE for coefficient (covers "Std. error", "Std. dev", CI, variance) ***
    SE_use_raw  = mapply(se_beta_from_error, Error, Std._error, Upper, Lower),
    SE_use      = ifelse(!is.na(SE_use_raw), abs(SE_use_raw), SE_use_raw)
  ) %>%
  mutate(
    df_hat    = ifelse(!is.na(N.value) & N.value > 2, N.value - 2, NA_real_),
    t_from_se = ifelse(!is.na(beta) & !is.na(SE_use) & SE_use > 0, beta / SE_use, NA_real_),
    t_from_p  = ifelse(!is.na(p_use) & !is.na(df_hat) & p_use > 0 & p_use < 1,
                       stats::qt(1 - p_use/2, df = df_hat), NA_real_),
    t_from_p  = ifelse(!is.na(t_from_p) & !is.na(beta), sign(beta) * t_from_p, t_from_p),
    tval      = dplyr::coalesce(t_from_se, t_from_p),
    r_raw     = ifelse(!is.na(tval) & !is.na(df_hat) & df_hat > 0,
                       tval / sqrt(tval^2 + df_hat), NA_real_)
  ) %>%
  mutate(
    g    = hedges_from_r(r_raw, N.value)$g,
    se_g = hedges_from_r(r_raw, N.value)$se_g
  ) %>%
  transmute(rowid, g, se_g, beta, r_raw)

## ---------- C) Join effects back to the FULL dataset & map required columns ----------
Hedge_B <- bind_rows(B_corr, B_reg)

B_comp_both <- B %>%
  left_join(Hedge_B, by = "rowid") %>%
  mutate(
    sample.size = suppressWarnings(as.numeric(N.value)),
    es  = g,
    se  = se_g,
    var = ifelse(!is.na(se), se^2, NA_real_),
    ci.lo = ifelse(!is.na(es) & !is.na(se), es - 1.96 * se, NA_real_),
    ci.hi = ifelse(!is.na(es) & !is.na(se), es + 1.96 * se, NA_real_)
  ) %>%
  select(-rowid)

## Result:
## B_comp = original zooALL columns + [es (Hedges g), se, sample.size, var, ci.lo, ci.hi]

```

#### Scatterplot unstandardised

```{r}
library(ggplot2)

B_comp = B_comp_both %>% 
  filter(Type %in% c("Unstandardised coefficient","Correlation coefficient"),
         !Statistical_method %in% correlation_methods)

if(is.null(B_comp$Correlation)){B_comp$Correlation = B_comp$Value}

# Scatter plot showing the relationship between correlation coefficient and effect size
B_comp_scatter <- ggplot(data = B_comp %>% filter(!is.na(es)), aes(x = Correlation, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Unstandardised coefficient") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Correlation, data = B_comp)
residuals <- model$residuals
outliers <- B_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
B_comp_scatter <- B_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Correlation, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Correlation, y = es, label = as.character(study)), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(B_comp_scatter)
```

#### Scatterplot correlation simple

```{r}

library(ggplot2)
B_corr = B_comp_both %>% filter(Type=="Correlation coefficient",
                                Statistical_method %in% correlation_methods)
if(is.null(B_corr$Correlation)){B_corr$Correlation = B_corr$Value}

# Scatter plot showing the relationship between correlation coefficient and effect size
B_corr_scatter <- ggplot(data = B_corr, aes(x = Correlation, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Correlation coefficient") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Correlation, data = B_corr)
residuals <- model$residuals
outliers <- B_corr[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
B_corr_scatter <- B_corr_scatter + 
  geom_point(data = outliers, 
             aes(x = Correlation, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Correlation, y = es, label = as.character(study)), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(B_corr_scatter)
```

### Spearman & Pearson correlaiton

```{r Effect sizes for Spearman & Pearson correlaiton, include=TRUE}

# # Subset data to include only Spearman and Pearson correlations
# R <- subset(zooALL, zooALL$Statistical_method %in% c("Spearman rank correlation", "Pearson correlation"))
# R.1 <- R
# 
# # Calculate Cohen's D effect size
# R_d <- cohens_d(R.1$Value)
# R.1$N.value <- as.numeric(R.1$N.value) 
# R.2 <- cbind(R.1, R_d) # Bind Dataframe and Cohen's D effect size calculation
# 
# # Calculate Hedge's G from Cohen's D
# Hedge_R <- as.data.frame(hedges_g(d = R.2$R_d, totaln = R.2$N.value))
# names(Hedge_R)[names(Hedge_R) == "hedges_g(d = R.2$R_d, totaln = R.2$N.value)"] <- "es" # Change name to 'es'
# 
# # Combine dataframes
# R_comp <- cbind(R.2, Hedge_R)
# names(Hedge_R)[names(Hedge_R) == "R$study"] <- "study" # Change name to 'study' to allow datasets to be combined later
# Hedge_R <- cbind(R.2$study, Hedge_R)
# names(Hedge_R)[names(Hedge_R) == "R.2$study"] <- "study"
# 
# # Unit scaling and sanity checks
# names(R_comp)[names(R_comp) == "Value"] <- "Correlation"

library(dplyr)

# Convert correlations to Hedges' g effect sizes
R_comp <- zooALL %>%
  filter(Statistical_method %in% c("Spearman rank correlation", "Pearson correlation")) %>% # correlation methods only
  transmute(
    study,
    r = as.numeric(Value),          # correlation coefficient
    n = as.numeric(N.value),        # sample size
    Value = Value,
    N.value = N.value,
    P.value_general = P.value_general,
    P.value_specific = P.value_specific,
    row_unique = row_unique
  ) %>%
  filter(!is.na(r), !is.na(n), n > 3, abs(r) < 1) %>% # quality filtering
  mutate(
    d  = 2 * r / sqrt(1 - r^2),     # Cohen's d from correlation: d = 2r/√(1-r²)
    J  = 1 - 3 / (4*n - 9),         # Hedges' correction factor
    g  = J * d,                      # Hedges' g (corrected d)
    se_d = 2 / sqrt((1 - r^2) * (n - 3)), # SE of Cohen's d
    se_g = J * se_d,                 # SE of Hedges' g
    es = g,                          # effect size
    se = se_g,                       # standard error
    sample.size = n                  # for meta-analysis
  ) %>%
  select(study, Value, N.value, P.value_general, P.value_specific, row_unique, r, es, se, sample.size)



```

#### Scatterplot spearman/pearson

```{r}
library(ggplot2)

if(is.null(R_comp$Correlation)){R_comp$Correlation = R_comp$Value}

# Scatter plot showing the relationship between correlation coefficient and effect size
R_comp_scatter <- ggplot(data = R_comp, aes(x = Correlation, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Correlation coefficient (r)") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Correlation, data = R_comp)
residuals <- model$residuals
outliers <- R_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
R_comp_scatter <- R_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Correlation, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Correlation, y = es, label = study), 
            position = position_jitter(width = 0.1, height = 0.5), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(R_comp_scatter)
```

### Ratios

```{r Effect sizes for Ratios, include=TRUE}

# Subset data to include only ratios
Ratios <- subset(zooALL, zooALL$Type %in% c("Relative risk", "Risk ratio", "Incidence rate ratio", "Prevalence ratio", "Hospitalisation rate ratio"))

# Convert 95% confidence intervals to Standard error
Ratios <- subset(Ratios, Ratios$Error %in% c("95% Confidence", "95% Credible"))

# Convert to numeric
Ratios$Value <- as.numeric(Ratios$Value)
Ratios$N.value <- as.numeric(Ratios$N.value)
Ratios$Upper <- as.numeric(Ratios$Upper)
Ratios$Lower <- as.numeric(Ratios$Lower)

# Calculate Standard error
Ratios$Std._error <- (Ratios$Upper - Ratios$Value) / 3.92
Ratios <- Ratios[complete.cases(Ratios$Upper), ]

# Replace NA values in N.value with the median
Ratios$N.value[is.na(Ratios$N.value)] <- median(Ratios$N.value, na.rm = TRUE)

# Convert odds ratios to Hedge's G effect size
Hedge_Ratios <- do.call(rbind, lapply(1:nrow(Ratios), function(i) {
  convert_or2d(
    or = Ratios$Value[i],
    se = Ratios$Std._error[i],
    totaln = Ratios$N.value[i],
    es.type = "g",
    study = Ratios$study[i]
  )
}))

# Convert to a dataframe and remove errors
Hedge_Ratios <- as.data.frame(Hedge_Ratios)
Hedge_Ratios <- Hedge_Ratios[Hedge_Ratios$se != 0, ]

# Unit scaling and sanity checks
Ratios_comp <- merge(Ratios, Hedge_Ratios, by = "study")
Ratios_comp$es <- unlist(Ratios_comp$es)
```

#### Scatterplot ratios

```{r}

# Initial scatter plot showing the relationship between ratios and effect size
Ratios_comp_scatter <- ggplot(data = Ratios_comp, aes(x = Value, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Ratios") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Value, data = Ratios_comp)
residuals <- model$residuals
outliers <- Ratios_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
Ratios_comp_scatter <- Ratios_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot with outliers
print(Ratios_comp_scatter)
```

### Z-scores

```{r Effect sizes for Z-scores, include=TRUE}

# Subset data to include only Z-scores
z <- subset(zooALL, zooALL$Type %in% c("Z-score"))
z$N.value <- as.numeric(z$N.value) # Convert to numeric

# Convert Z-scores to Cohen's D and then to Hedge's G
Hedge_z <- convert_z2r(z$Value)
Hedge_z <- cohens_d(Hedge_z)
Hedge_z <- as.data.frame(hedges_g(d = Hedge_z, totaln = z$N.value))

# Rename columns for consistency
Hedge_z <- cbind(z$study, Hedge_z)
names(Hedge_z)[names(Hedge_z) == "z$study"] <- "study"
names(Hedge_z)[names(Hedge_z) == "hedges_g(d = Hedge_z, totaln = z$N.value)"] <- "es"

# Unit scaling and sanity checks
z_comp <- merge(z, Hedge_z, by = "study")
```

#### Scatterplot z-score

```{r}
library(ggplot2)

# Scatter plot showing the relationship between Z-score and effect size
z_comp_scatter <- ggplot(data = z_comp, aes(x = Value, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Z-score") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Value, data = z_comp)
residuals <- model$residuals
outliers <- z_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
z_comp_scatter <- z_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot with outliers
print(z_comp_scatter)
```

### T-test

```{r Effect sizes for T-test, include=TRUE}

# Subset data to include only T-values
t <- subset(zooALL, zooALL$Type %in% c("T-value"))
# Hedge_t<-as.data.frame(esc_t(t=t$Value, p=t$P.value_specific, totaln = t$N.value, es.type = "g", study = t$study))

library(dplyr)
library(purrr)
library(tidyr)
dat = t 

Hedge_t <- dat %>%
  mutate(.row = row_number()) %>%
  pmap_dfr(function(Value, P.value_specific, N.value, study, row_unique, .row, ...) {
    # If essential inputs missing, return NA but keep identifiers
    if (is.na(Value) || is.na(N.value)) {
      return(tibble(.row = .row, row_unique = row_unique, es = NA_real_, se = NA_real_))
    }

    # Safe call to esc_t (returns a data.frame-like object)
    res <- tryCatch(
      as_tibble(as.data.frame(
        esc::esc_t(t = Value,
                   p = P.value_specific,
                   totaln = N.value,
                   es.type = "g",
                   study = study)
      )),
      error = function(e) {
        # on error, return NA placeholders (match expected columns if you know them)
        tibble(es = NA_real_, se = NA_real_)
      }
    )

    # attach identifiers and return
    res %>% mutate(.row = .row, row_unique = row_unique, Value = Value)
  })

```

#### Scatteplot T test

```{r}

# Scatter plot showing the relationship between Z-score and effect size
Hedge_t_scatter <- ggplot(data = Hedge_t, aes(x = Value, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("T") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Value, data = Hedge_t)
residuals <- model$residuals
outliers <- Hedge_t[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
Hedge_t_scatter <- Hedge_t_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot with outliers
print(Hedge_t_scatter)
```

### Chi-square

```{r Effect sizes for Chi-square, include=TRUE}

# Subset data to include only Chi-square values
Chi <- subset(zooALL, zooALL$Type %in% c("Chi-square"))

# Convert Chi-square values to Hedge's G effect size
Hedge_chi<-as.data.frame(esc_chisq(chisq = Chi$Value, p = Chi$P.value_specific, totaln = Chi$N.value, es.type = c("g"), study = Chi$study)) 

Hedge_chi <- Chi %>%
  mutate(.row = row_number()) %>%
  pmap_dfr(function(Value, P.value_specific, N.value, study, row_unique, .row, ...) {
    # keep identifiers even when inputs are missing
    if (is.na(Value) || is.na(N.value)) {
      return(tibble(.row = .row, row_unique = row_unique, es = NA_real_, se = NA_real_))
    }
    # safe call to esc_chisq
    res <- tryCatch(
      as_tibble(as.data.frame(
        esc::esc_chisq(chisq = Value,
                       p = P.value_specific,
                       totaln = N.value,
                       es.type = "g",
                       study = study)
      )),
      error = function(e) {
        # return placeholders with correct column names if there is an error
        tibble(es = NA_real_, se = NA_real_)
      }
    )

    # attach identifiers and return
    res %>% mutate(.row = .row, row_unique = row_unique)
  })
```

### F-statistic

```{r Effect sizes for  F-statistic, include=TRUE}

# Subset data to include only F-statistics
FS <- subset(zooALL, zooALL$Type %in% c("F-statistic"))

# Convert columns to numeric
FS$Value <- as.numeric(FS$Value)
FS$N.value <- as.numeric(FS$N.value)

# Convert F-statistics to Hedge's G effect size
Hedge_F <- do.call(rbind, lapply(1:nrow(FS), function(i) {
  esc_f(
    f = FS$Value[i],
    totaln = FS$N.value[i],
    es.type = "g",
    study = FS$study[i]
  )
}))

# Convert to a dataframe and unlist the effect size
Hedge_F <- as.data.frame(Hedge_F)
Hedge_F$es <- unlist(Hedge_F$es)

# Unit scaling and sanity checks
F_comp <- merge(FS, Hedge_F, by = "study")
```

#### Scatterplot F-statistic

```{r}
library(ggplot2)

# Scatter plot showing the relationship between F-statistic and effect size
F_comp_scatter <- ggplot(data = F_comp, aes(x = Value, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("F-statistic") +
  ylab("Effect size (g)") +
  theme_classic() +
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF")) +  # Custom color
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16)) +  # Custom shape
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(F_comp_scatter)
```

### Percentage change

```{r Effect sizes for Percentage change, include=TRUE, error=TRUE}

# Subset data to include only Percent change
p <- subset(zooALL, zooALL$Type %in% "Percent change")

# Convert % change to a rough b coefficient and limits
p$B <- (p$Value / 100)
p$L <- (p$Lower / 100)
p$U <- (p$Upper / 100)

# Convert 95% Confidence interval to Standard deviation
p_95 <- subset(p, p$Error %in% c("95% Confidence"))

# Convert relevant columns to numeric
p$B <- as.numeric(p$B)
p_95$U <- as.numeric(p_95$U)
p_95$L <- as.numeric(p_95$L)
p_95$N.value <- as.numeric(p_95$N.value)
p$Std._error <- as.numeric(p$Std._error)

# Calculate standard deviation from 95% CI
p_95_sd <- as.data.frame((sqrt(p_95$N.value) * (p_95$U - p_95$L)) / 3.92)
p_95 <- cbind(p_95, p_95_sd)
names(p_95)[names(p_95) == "(sqrt(p_95$N.value) * (p_95$U - p_95$L))/3.92"] <- "Std_dev"

# Convert Standard error to Standard Deviation
p_SE <- subset(p, p$Error %in% "Std. error")

# Convert relevant columns to numeric
p_SE$U <- as.numeric(p_SE$U)
p_SE$L <- as.numeric(p_SE$L)
p_SE$N.value <- as.numeric(p_SE$N.value)
p_SE$Std._error <- as.numeric(p_SE$Std._error)

# Calculate standard deviation from Standard error
p_SE_sd <- as.data.frame(p_SE$Std._error * sqrt(p_SE$N.value))
p_SE <- cbind(p_SE, p_SE_sd)
names(p_SE)[names(p_SE) == "p_SE$Std._error * sqrt(p_SE$N.value)"] <- "Std_dev"

# Combine the two dataframes
p_Stdev <- rbind(p_95, p_SE)

# Calculate Hedge's G effect size
Hedge_p <- as.data.frame(esc_beta(beta = p_Stdev$B, sdy = p_Stdev$Std_dev, grp1n = p_Stdev$N.value / 2, grp2n = p_Stdev$N.value / 2, es.type = "g", study = p_Stdev$study))
Hedge_p <- Hedge_p[!is.na(Hedge_p$se), ]

# Unit scaling and sanity checks
p_comp <- merge(p_Stdev, Hedge_p, by = "study")
names(p_comp)[names(p_SE) == "B"] <- "Correlation"

# Test for 95% CI
p_test_ci <- subset(p_comp, Error %in% c("95% Confidence"))
p_test_ci <- sample_n(p_test_ci, 5)
```

#### Scatterplot percentage change

```{r}
library(ggplot2)
p_comp <- merge(p_Stdev, Hedge_p, by = "study")
# Scatter plot showing the relationship between correlation coefficient and effect size
p_comp_scatter <- ggplot(data = p_comp, aes(x = Value, y = es)) +
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_line(alpha = 0.3) +  # Add line
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Percentage change") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Value, data = p_comp)
residuals <- model$residuals
outliers <- p_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
p_comp_scatter <- p_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0, height = 0.025), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(p_comp_scatter)
```

### R-squared

```{r Effect sizes for R-squared, include=TRUE, error=TRUE}

# Subset data to include only R-squared values
R_square <- subset(zooALL, zooALL$Type %in% "R square")

# Convert R-squared to Cohen's f^2 effect size
R_square$R.squared <- as.numeric(R_square$R.squared) # Ensure R.squared is numeric
f2 <- R_square$R.squared / (1 - R_square$R.squared)

# Convert Cohen's f^2 to Hedge's g effect size
g <- as.data.frame(sqrt(f2) * 1.5)

# Combine with study and direction information
Hedge_R_square <- cbind(R_square$study, g, R_square$Direction)

# Rename columns for consistency
names(Hedge_R_square)[names(Hedge_R_square) == "sqrt(f2) * 1.5"] <- "es"
names(Hedge_R_square)[names(Hedge_R_square) == "R_square$study"] <- "study"
names(Hedge_R_square)[names(Hedge_R_square) == "R_square$Direction"] <- "Direction"

# Change sign of effect size based on the direction of the relationship
Hedge_R_square$es[Hedge_R_square$Direction == "decrease"] <- -Hedge_R_square$es[Hedge_R_square$Direction == "decrease"]
Hedge_R_square<-Hedge_R_square[,-3]

# Unit scaling and sanity checks
Rsquare_comp <- merge(Hedge_R_square, R_square, by = "study")
```

#### Scatterplot R-squared

```{r}
library(ggplot2)

# Scatter plot showing the relationship between R-squared and effect size
Rsquare_comp_scatter <- ggplot(data = Rsquare_comp, aes(x = Value, y = es)) +
  geom_line() +  # Add line
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("R-squared") +
  ylab("Effect size (g)") +
  theme_classic()

# Assess outliers using residuals
model <- lm(es ~ Value, data = Rsquare_comp)
residuals <- model$residuals
outliers <- Rsquare_comp[abs(residuals) > 2 * sd(residuals), ]

# Add outliers to scatter plot
Rsquare_comp_scatter <- Rsquare_comp_scatter + 
  geom_point(data = outliers, 
             aes(x = Value, y = es, color = "Outlier", shape = "Outlier"), 
             size = 3.5) +  # Add slightly larger points for outliers
  geom_text(data = outliers, 
            aes(x = Value, y = es, label = study), 
            position = position_jitter(width = 0.6, height = 0.2), 
            alpha = 0.9, 
            size = 3.5) +  # Add text labels only for outliers
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF", "Outlier" = "#E54E21FF")) +  # Custom colors
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16, "Outlier" = 16)) +  # Custom shapes
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(Rsquare_comp_scatter)
```

### Bivariate regression analysis estimate

```{r Effect sizes for Bivariate regression analysis estimate, include=TRUE, error=TRUE}
# Subset data to include only Estimate values
Est <- subset(zooALL, zooALL$Type %in% "Estimate")

# Convert to numeric
Est$Value <- as.numeric(Est$Value)
Est$N.value <- as.numeric(Est$N.value)

# Calculate Hedge's G effect size
Hedge_est <- as.data.frame(hedges_g(d = Est$Value, totaln = Est$N.value))

# Combine with study information
Hedge_est <- cbind(Est$study, Hedge_est)

# Rename columns for consistency
names(Hedge_est)[names(Hedge_est) == "hedges_g(d = Est$Value, totaln = Est$N.value)"] <- "es"
names(Hedge_est)[names(Hedge_est) == "Est$study"] <- "study"

# Merge datasets
Est_comp <- merge(Est, Hedge_est, by = "study")

# Scatter plot showing the relationship between Estimate and effect size
Est_comp_scatter <- ggplot(data = Est_comp, aes(x = Value, y = es, label = study)) +
  geom_point() +
  xlab("Estimate") +
  ylab("Effect size (g)") +
  theme_classic() +
  geom_line() +
  geom_text(aes(label = study), vjust = 1) +
  geom_smooth(method = "lm", se = FALSE)

# Display scatter plot
print(Est_comp_scatter)
```

#### Scatterplot bivariate regression

```{r}
library(ggplot2)

# Scatter plot showing the relationship between Estimate and effect size
Est_comp_scatter <- ggplot(data = Est_comp, aes(x = Value, y = es)) +
  geom_smooth(method = "lm", se = FALSE, color = "#54D8B1FF") +  # Add smooth line with custom color
  geom_line(alpha = 0.3) +  # Add line
  geom_point(aes(color = "Observation", shape = "Observation"), size = 3, alpha = 0.5) +  # Add main data points
  xlab("Estimate") +
  ylab("Effect size (g)") +
  theme_classic() +
  scale_color_manual(name = "", 
                     values = c("Observation" = "#175149FF")) +  # Custom color
  scale_shape_manual(name = "", 
                     values = c("Observation" = 16)) +  # Custom shape
  theme(legend.position = "right",
        legend.text = element_text(size = 12),  # Increase legend text size
        axis.text.x = element_text(size = 12),  # Increase x-axis text size
        axis.text.y = element_text(size = 12),  # Increase y-axis text size
        axis.title.x = element_text(size = 14), # Increase x-axis title size
        axis.title.y = element_text(size = 14)) # Increase y-axis title size

# Display scatter plot
print(Est_comp_scatter)
```

## Combine plots

### Add annotations based on the significance of regression

```{r}

names(OR_comp)
names(Beta_comp)
names(B_comp)
names(B_corr)
names(R_comp)
names(Ratios_comp)
names(z_comp)
names(Hedge_t)
names(Hedge_chi)
names(F_comp)
names(p_comp)
names(Rsquare_comp)
names(Est_comp)

```

```{r}
# Annotate plots based on model significance

# List of models to evaluate
models <- list(
  lm1 = lm(es ~ Value, data = OR_comp),
  lm2 = lm(es ~ Correlation, data = Beta_comp),
  lm3 = lm(es ~ Correlation, data = B_comp),
  lm4 = lm(es ~ Correlation, data = R_comp),
  lm5 = lm(es ~ Value, data = Ratios_comp),
  lm6 = lm(es ~ Value, data = z_comp),
  lm7 = lm(es ~ Value, data = F_comp),
  lm8 = lm(es ~ Value, data = Est_comp),
  lm9 = lm(es ~ Value, data = p_comp),
  lm10 = lm(es ~ Value, data = Rsquare_comp),
  lm11 = lm(es ~ Value, data = Hedge_t),
  lm12 = lm(es ~ Value, data = B_corr))

# Function to check significance
check_significance <- function(model) {
  p_value <- summary(model)$coefficients[2, 4]  # Extract the p-value for the predictor
  if (p_value < 0.001) {
    return("***")
  } else if (p_value < 0.01) {
    return("**")
  } else if (p_value < 0.05) {
    return("*")
  } else {
    return("")
  }
}

model_significance <- lapply(models, check_significance)

# Combine indivisual datasets [OR_comp, Beta_comp, B_comp, R_comp, Ratios_comp, z_comp, F_comp, Est_comp, p_comp, Rsquare_comp]:
# zooALL_raw = left_join(OR_comp, Beta_comp, by = "study")
```

# Combine datasets

```{r}
library(dplyr)
library(purrr)

cols_n = c("row_unique","es", "se", "var", "ci.lo", "ci.hi","weight", "sample.size")
(cols_n)[!cols_n %in% names(OR_comp)] #has totaln
(cols_n)[!cols_n %in% names(Beta_comp)]
(cols_n)[!cols_n %in% names(B_comp)]
(cols_n)[!cols_n %in% names(B_corr)]
(cols_n)[!cols_n %in% names(R_comp)]
(cols_n)[!cols_n %in% names(Ratios_comp)] #has totaln
(cols_n)[!cols_n %in% names(z_comp)]
(cols_n)[!cols_n %in% names(Hedge_t)]
(cols_n)[!cols_n %in% names(Hedge_chi)]
(cols_n)[!cols_n %in% names(F_comp)]
(cols_n)[!cols_n %in% names(p_comp)]
(cols_n)[!cols_n %in% names(Rsquare_comp)]
(cols_n)[!cols_n %in% names(Est_comp)]

library(dplyr)
library(purrr)
library(rlang)
library(tibble)
library(stringr)

# ---- config ----
target_cols <- c("row_unique","es","se","weight","sample.size","var","ci.lo","ci.hi")
numeric_cols <- setdiff(target_cols, "row_unique")

# Name your components so diagnostics are readable
components <- list(
  OR_comp      = OR_comp,
  Beta_comp    = Beta_comp,
  B_comp       = B_comp,
  B_corr       = B_corr,
  R_comp       = R_comp,
  Ratios_comp  = Ratios_comp,
  z_comp       = z_comp,
  Hedge_t      = Hedge_t,
  Hedge_chi    = Hedge_chi,
  F_comp       = F_comp,
  p_comp       = p_comp,
  Rsquare_comp = Rsquare_comp,
  Est_comp     = Est_comp
)

# ---- helpers ----

# simplify a vector or list-column to numeric (taking the first element if length>1)
as_numeric_scalar <- function(x) {
  if (is.list(x)) {
    vapply(x, function(el) {
      if (length(el) == 0) return(NA_real_)
      suppressWarnings(as.numeric(el[[1]]))
    }, numeric(1))
  } else if (is.factor(x)) {
    suppressWarnings(as.numeric(as.character(x)))
  } else if (is.character(x) || is.numeric(x) || is.integer(x) || is.logical(x)) {
    suppressWarnings(as.numeric(x))
  } else {
    # unknown type -> NA
    rep(NA_real_, length(x))
  }
}

# standardise names, select target cols, coerce types
standardise_effect_cols <- function(df) {
  if (is.null(df)) return(tibble(row_unique = character()))
  
  nm <- names(df)
  if ("w" %in% nm && !("weight" %in% nm))            df[["weight"]]      <- df[["w"]]
  if ("totaln" %in% nm && !("sample.size" %in% nm))  df[["sample.size"]] <- df[["totaln"]]
  
  out <- dplyr::select(df, dplyr::any_of(target_cols))
  
  # Ensure row_unique exists and is character
  if (!"row_unique" %in% names(out)) out$row_unique <- NA_character_
  out$row_unique <- as.character(out$row_unique)
  
  # Force numeric columns to be plain doubles (flatten list-cols if needed)
  for (cn in intersect(names(out), numeric_cols)) {
    out[[cn]] <- as_numeric_scalar(out[[cn]])
  }
  out
}

# left-join and fill NA using coalesce(), after we ensured types are numeric
fill_join <- function(acc, df) {
  if (!"row_unique" %in% names(df)) return(acc)
  df2 <- dplyr::select(df, dplyr::all_of(intersect(names(df), target_cols)))
  out <- dplyr::left_join(acc, df2, by = "row_unique", suffix = c("", ".new"))
  for (nm in numeric_cols) {
    new_nm <- paste0(nm, ".new")
    if (new_nm %in% names(out)) {
      out[[nm]] <- dplyr::coalesce(out[[nm]], out[[new_nm]])
      out[[new_nm]] <- NULL
    }
  }
  out
}

# diagnostics for each component
diagnose_component <- function(df, name) {
  cat("\n==============================\n")
  cat("Component:", name, "\n")
  if (is.null(df)) { cat("NULL\n"); return(invisible(NULL)) }
  
  cat("Rows:", nrow(df), "\n")
  has_ru <- "row_unique" %in% names(df)
  cat("Has row_unique:", has_ru, "\n")
  if (has_ru) {
    nunq <- dplyr::n_distinct(df$row_unique)
    cat("Distinct row_unique:", nunq, "\n")
    dups <- sum(duplicated(df$row_unique))
    cat("Duplicated row_unique:", dups, "\n")
  }
  
  present <- intersect(names(df), target_cols)
  missing <- setdiff(target_cols, names(df))
  cat("Present target cols:", paste(present, collapse=", "), "\n")
  cat("Missing target cols:", paste(missing, collapse=", "), "\n")
  
  # show classes of target columns
  cls <- sapply(df[, intersect(names(df), target_cols), drop = FALSE], function(x) class(x)[1])
  if (length(cls)) {
    cat("Column classes:\n")
    print(cls)
  }
  
  # list-cols among target columns
  list_cols <- names(df)[sapply(df, is.list) & names(df) %in% target_cols]
  if (length(list_cols)) {
    cat("List-columns among targets:", paste(list_cols, collapse=", "), "\n")
    # show a tiny sample of lengths
    for (lc in list_cols) {
      lens <- lengths(df[[lc]])
      cat(sprintf("  %s: max length=%d; non-scalar rows=%d\n", lc, max(lens, na.rm = TRUE), sum(lens > 1, na.rm = TRUE)))
    }
  } else {
    cat("No list-columns among target cols.\n")
  }
}

# cross-component duplicate check (same row_unique in multiple components)
cross_component_dups <- function(comp_list) {
  keys <- imap(comp_list, ~ tibble(component = .y, row_unique = as.character(.x$row_unique))) %>%
    bind_rows() %>% filter(!is.na(row_unique))
  dup_keys <- keys %>% count(row_unique, sort = TRUE) %>% filter(n > 1)
  list(dups = dup_keys, keys = keys)
}

# ---- run diagnostics ----
walk2(components, names(components), diagnose_component)

dup_info <- cross_component_dups(components)
cat("\n=====================================\n")
cat("row_unique present in multiple components:", nrow(dup_info$dups), "\n")
if (nrow(dup_info$dups)) {
  cat("Top overlapping keys (head):\n")
  print(head(dup_info$dups, 10))
}

# ---- standardise and join ----
components_std <- imap(components, ~ {
  out <- standardise_effect_cols(.x)
  # optional: quick summary after standardisation
  bad_types <- names(out)[names(out) %in% numeric_cols & !sapply(out[numeric_cols[numeric_cols %in% names(out)]], is.double)]
  if (length(bad_types)) {
    cat("\n[Warn] After standardise, still non-numeric in", .y, ":", paste(bad_types, collapse=", "), "\n")
  }
  out
})

# Ensure zooALL_raw has the target columns (as doubles) to receive coalesced values
for (cn in setdiff(target_cols, "row_unique")) {
  if (!cn %in% names(zooALL_raw)) zooALL_raw[[cn]] <- NA_real_
  # coerce to double if needed
  if (!is.double(zooALL_raw[[cn]])) zooALL_raw[[cn]] <- suppressWarnings(as.numeric(zooALL_raw[[cn]]))
}
# row_unique as character
zooALL_raw$row_unique <- as.character(zooALL_raw$row_unique)

# Join and fill
zooALL_raw <- reduce(components_std, fill_join, .init = zooALL_raw)

#Optional derivations
 zooALL_raw <- zooALL_raw %>%
   mutate(
     var    = coalesce(var, se^2),
     #weight = coalesce(weight, if_else(!is.na(var) & var > 0, 1/var, NA_real_)),
     ci.lo  = coalesce(ci.lo, if_else(!is.na(es) & !is.na(se), es - 1.96 * se, NA_real_)),
     ci.hi  = coalesce(ci.hi, if_else(!is.na(es) & !is.na(se), es + 1.96 * se, NA_real_))
   )

# Safety: row count unchanged and unique keys preserved
stopifnot(nrow(zooALL_raw) == dplyr::n_distinct(zooALL_raw$row_unique))
cat("\n[OK] zooALL_raw joined successfully. Rows:", nrow(zooALL_raw), "\n")

View(zooALL_raw %>% select(row_unique, General_Stats_Method, Statistical_method, Type, Value, N.value, Upper, Lower, Std._error, Error,P.value_general, P.value_specific, es, se) %>% filter(is.na(se)))

```
### SAVE COMBINED

```{r}
# Merge with initial dataset
## Get the ones wihout Values
 not_changed = df %>% filter(!row_unique %in% zooALL_raw$row_unique)
 dim(not_changed)[1] + dim(zooALL_raw)[1] == dim(zooALL_copy)[1]

## Rename Data ID column
zooALL_raw = zooALL_raw %>% rename(Data_ID = study)
## Check if all names match
all(names(zooALL_raw) %in% names(not_changed))

zooALL_final = rbind(zooALL_raw, not_changed)
write_csv(zooALL_final, here("data","dataset_final_g.csv"))
```

```{r}
vjust_value = 4
custom_size = 7
custom_alpha = 0.8
custom_color = "#AF4E24FF"
#"#C52E19FF", "#AC9765FF", "#54D8B1FF", "#B67C3BFF", "#175149FF", "#AF4E24FF"
OR_comp_scatter <- OR_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm1, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

Rsquare_comp_scatter <- Rsquare_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm10, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

Beta_comp_scatter <- Beta_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm2, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

B_comp_scatter <- B_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm3, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

R_comp_scatter <- R_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm4, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

Ratios_comp_scatter <- Ratios_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm5, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

z_comp_scatter <- z_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm6, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

F_comp_scatter <- F_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm7, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

Est_comp_scatter <- Est_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm8, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

p_comp_scatter <- p_comp_scatter + 
  annotate("text", x = Inf, y = Inf, label = model_significance$lm9, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

Hedge_t_scatter <- Hedge_t_scatter +
  annotate("text", x = Inf, y = Inf, label = model_significance$lm11, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)

B_corr_scatter <- B_corr_scatter +
  annotate("text", x = Inf, y = Inf, label = model_significance$lm12, vjust = vjust_value, hjust = 1.1, size = custom_size, color = custom_color, alpha = custom_alpha)
```

```{r}
library(patchwork)
library(cowplot)

# Create the combined plot without legends
combined_plot <- (OR_comp_scatter + theme(legend.position = "none")) + 
  (Rsquare_comp_scatter + theme(legend.position = "none")) + 
  (Beta_comp_scatter + theme(legend.position = "none")) + 
  (B_comp_scatter + theme(legend.position = "none")) + 
  (R_comp_scatter + theme(legend.position = "none")) + 
  (Ratios_comp_scatter + theme(legend.position = "none")) + 
  (z_comp_scatter + theme(legend.position = "none")) + 
  (Hedge_t_scatter + theme(legend.position = "none")) + 
  (Est_comp_scatter + theme(legend.position = "none")) + 
  (p_comp_scatter + theme(legend.position = "none")) + 
  (B_corr_scatter + theme(legend.position = "none")) + 
  (F_comp_scatter + theme(legend.position = "none")) +
  plot_layout(ncol = 2, nrow = 6)

# Extract the legend from one of the plots
legend_plot <- get_legend(p_comp_scatter)

# Combine the combined plot and the legend
final_plot <- plot_grid(
  combined_plot,                # The main combined plot
  legend_plot,                  # The extracted legend
  ncol = 1,                     # Stack them vertically
  rel_heights = c(0.95, 0.06)   # Allocate more space to the plots, less to the legend
)

# Display the final plot
print(final_plot)

# Save the final plot
ggsave(here('outputs','combined_plot_NEWDATA_A4_70percent_signif_17SEP.png'), final_plot, width = 5.5, height = 6.7, units = "in", dpi = 300, scale = 1.7)
```

```{r}
#cite_packages(output = "file", out.dir = getwd(),out.format = "pdf")
```

```{r }
#pkgs <- cite_packages(output = "table", out.dir = ".")
#knitr::kable(pkgs)
```
